{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing linear regression and DFL-IO\n",
    "\n",
    "Steps comparing the linear and inverse-DFL models:\n",
    "1. Save two datasets\n",
    "4. Split each dataset into train and test sets\n",
    "5. Run DFL-IO with each training set, get weights $\\theta^{\\text{IO,close}}$ and $\\theta^{\\text{IO,wide}}$\n",
    "6. Run linear regression on training set, get weights $\\theta^{\\text{R,close}}$ and $\\theta^{\\text{R,wide}}$\n",
    "7. Compare all models on each training set, for each model:\n",
    "    1. Predict demand using weights and features\n",
    "    2. Solve MCFND with demand, get design variables $\\hat{y}$\n",
    "    3. Solve MCF-Flow with fixed design variables $\\hat{y}$, get $\\hat{x}$\n",
    "    4. Compare costs of $\\hat{y}, \\hat{x}$ with optimal $x^*, y^*$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using JuMP\n",
    "using Gurobi\n",
    "using LinearAlgebra\n",
    "using Distributions\n",
    "using Random\n",
    "using PDMats\n",
    "using MLJ\n",
    "using Tables\n",
    "using DataFrames\n",
    "using Plots, StatsPlots\n",
    "using JLD\n",
    "using CSV\n",
    "\n",
    "Random.seed!(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Revise\n",
    "\n",
    "includet(\"../models/forward.jl\")\n",
    "import .Forward as Forward\n",
    "\n",
    "includet(\"../models/inversedemand.jl\")\n",
    "import .InverseDemand as IODemand\n",
    "\n",
    "includet(\"../models/inverselinreg.jl\")\n",
    "import .InverseLinReg as IOLinReg\n",
    "\n",
    "includet(\"../datagen/data-generation.jl\")\n",
    "import .DataGeneration as DataGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Linear = @MLJ.load LinearRegressor pkg = \"MLJLinearModels\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DATA_PATH = \"../data/\"\n",
    "BASE_RESULTS_PATH = \"../results/\"\n",
    "\n",
    "CLOSE_DATA_NAME = \"close\"\n",
    "WIDE_DATA_NAME = \"wide\"\n",
    "\n",
    "function dataset_path(n_points)\n",
    "    return joinpath(BASE_DATA_PATH, \"data_$n_points.jld\") \n",
    "end\n",
    "\n",
    "function results_path(n_points)\n",
    "    return joinpath(BASE_RESULTS_PATH, \"results_$n_points.csv\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a smaller problem with 1 commodity and 2 possible arcs, one low-ish capacity ($C$) and one high ($\\infty$)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_params = Forward.Params(\n",
    "    n_paths=2, \n",
    "    n_commodities=1,\n",
    "    capacities=[100, 100000],\n",
    "    design_costs=[10, 100],\n",
    "    flow_costs=[10 100]',\n",
    "    enabled_flows=ones(Bool, (2, 1))\n",
    ")\n",
    "\n",
    "datagen_params = DataGen.DataGenParams(\n",
    "    weights=[1.5 -3 2], \n",
    "    noise_variance=[5.0^2]\n",
    ")\n",
    "\n",
    "inverse_params = IOLinReg.Params(\n",
    "    n_features=datagen_params.n_features, \n",
    "    forward_params=forward_params, \n",
    "    with_noise=true\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generation\n",
    "\n",
    "Generate two datasets using fixed weights $\\Theta$:\n",
    "- $\\mathcal{D}_{\\text{close}}$ with $\\mathbb{E}[d] = C$ \n",
    "- $\\mathcal{D}_{\\text{wide}}$ with $\\mathbb{E}[d] \\ll C$. \n",
    "\n",
    "Procedure for each data point in the dataset:\n",
    "1. Draw $\\phi_1, \\ldots, \\phi_{m-1} \\sim U(a, b)$ for some $a, b$\n",
    "2. Set $\\phi_m$ such that $\\sum_{i=1}^m \\theta_i \\phi_i = \\mathbb{E}[d]$\n",
    "3. Draw noise $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$ and compute $d = \\sum_{i=1}^m \\theta_i \\phi_i + \\epsilon$\n",
    "4. Solve MCFND for $d$ \n",
    "5. Datapoint $(\\phi, d, x^*, y^*) \\in \\mathcal{D}$\n",
    "\n",
    "Repeat dataset creation for several number of points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_points = [100, 1000, 3000, 10000]\n",
    "close_target_demand = 100\n",
    "wide_target_demand = 20\n",
    "\n",
    "gurobi_env = Gurobi.Env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in n_points\n",
    "    close_dataset = DataGen.generate_dataset(forward_params, datagen_params, n_points=n, target_demand=close_target_demand, gurobi_env=gurobi_env)\n",
    "    wide_dataset = DataGen.generate_dataset(forward_params, datagen_params, n_points=n, target_demand=wide_target_demand, gurobi_env=gurobi_env)\n",
    "\n",
    "    save(dataset_path(n), CLOSE_DATA_NAME, close_dataset, WIDE_DATA_NAME, wide_dataset, compress=true)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a training function and a prediction function for each model:\n",
    "- `train_{model_type}_model` takes in a training dataset of `IOLinReg.SolutionPoint`s, and returns a trained model\n",
    "- `predict_{model_type}_model` takes a model of the correct type and a test dataset of `IOLinReg.SolutionPoint`s, and returns a vector of predicted demands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function load_dataset(n_points)\n",
    "    dataset = JLD.load(dataset_path(n_points))\n",
    "\n",
    "    return dataset[CLOSE_DATA_NAME], dataset[WIDE_DATA_NAME]\n",
    "end\n",
    "\n",
    "function convert_dataset_to_mlj(dataset)\n",
    "    features = DataFrame(vcat(map(sol -> sol.linreg_features', dataset)...), :auto)\n",
    "    demands = vcat(map(sol -> sol.actual_demands, dataset)...)\n",
    "\n",
    "    return features, demands\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DFL-IO model training and prediction functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function train_inverse_model(training_dataset)\n",
    "    model = IOLinReg.create_problem(inverse_params, training_dataset, gurobi_env=gurobi_env)\n",
    "\n",
    "    return IOLinReg.solve_problem!(model, inverse_params)\n",
    "end\n",
    "\n",
    "function predict_inverse_model(inverse_solution, test_dataset)\n",
    "    features = map(row -> row.linreg_features, test_dataset)\n",
    "    predict = f -> IOLinReg.predict_inverse_model(inverse_solution, f)\n",
    "\n",
    "    return vcat(map(predict, features)...)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression model training and prediction functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function train_linear_model(training_dataset)\n",
    "    features, demands = convert_dataset_to_mlj(training_dataset)\n",
    "\n",
    "    model = Linear()\n",
    "    mach = machine(model, features, demands)\n",
    "    fit!(mach)\n",
    "\n",
    "    return mach\n",
    "end\n",
    "\n",
    "function predict_linear_model(linreg_machine, test_dataset)\n",
    "    features, _ = convert_dataset_to_mlj(test_dataset)\n",
    "    return predict(linreg_machine, features)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define an evaluation procedure `evaluate_model_on_dataset` as described in point 5 of the introduction. Takes a train and test dataset, and a training and prediction function for a given model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function evaluate_model_on_dataset(train_data, test_data, train_model, make_predictions; gurobi_env=nothing)  \n",
    "    trained_model = train_model(train_data)\n",
    "    predicted_demands = make_predictions(trained_model, test_data)\n",
    "    designed_network = compute_predicted_network_design(forward_params, predicted_demands, gurobi_env=gurobi_env)\n",
    "\n",
    "    return compute_flow_problem_results(forward_params, test_data, designed_network)\n",
    "end\n",
    "\n",
    "function compute_predicted_network_design(forward_params, predicted_demands; gurobi_env=nothing)\n",
    "    solve_mcfnd = d -> Forward.create_and_solve_problem(forward_params, d, silent=true, gurobi_env=gurobi_env)\n",
    "    return map(d -> solve_mcfnd(d).z_sol, predicted_demands) \n",
    "end\n",
    "\n",
    "function compute_flow_problem_results(forward_params, test_dataset, predicted_z_sols; gurobi_env=gurobi_env)\n",
    "    actual_demands = map(row -> row.actual_demands, test_dataset)\n",
    "    solve_flow = (d, z_sol) -> Forward.create_and_solve_flow_problem(forward_params, d, z_sol, silent=true, gurobi_env=gurobi_env)\n",
    "    \n",
    "    forward_solutions = map(solve_flow, actual_demands, predicted_z_sols)\n",
    "    task_losses = map(sol -> sol.objective_value, forward_solutions)\n",
    "    recourse_flow = map(sol -> sol.recourse_flow, forward_solutions)\n",
    "\n",
    "    return DataFrame(task_losses=task_losses, recourse_flow=recourse_flow)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model comparison pipeline and result cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a given full dataset of `n_points`, obtain the results of the DFL-IO and the Linear Regression model over the close and wide datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function compare_models(n_points; test_train_split=0.7)\n",
    "    close_dataset, wide_dataset = load_dataset(n_points)\n",
    "\n",
    "    close_train, close_test = partition(close_dataset, test_train_split)\n",
    "    wide_train, wide_test =  partition(close_dataset, test_train_split)\n",
    "\n",
    "    close_io_results = evaluate_model_on_dataset(close_train, close_test, train_inverse_model, predict_inverse_model, gurobi_env=gurobi_env)\n",
    "    close_linreg_results = evaluate_model_on_dataset(close_train, close_test, train_linear_model, predict_linear_model, gurobi_env=gurobi_env)\n",
    "    wide_io_results = evaluate_model_on_dataset(wide_train, wide_test, train_inverse_model, predict_inverse_model, gurobi_env=gurobi_env)\n",
    "    wide_linreg_results = evaluate_model_on_dataset(wide_train, wide_test, train_linear_model, predict_linear_model, gurobi_env=gurobi_env)\n",
    "\n",
    "    return vcat(\n",
    "        specify_model_and_data(close_io_results, \"close\", \"io\"),\n",
    "        specify_model_and_data(close_linreg_results, \"close\", \"linreg\"),\n",
    "        specify_model_and_data(wide_io_results, \"wide\", \"io\"),\n",
    "        specify_model_and_data(wide_linreg_results, \"wide\", \"linreg\")\n",
    "    )\n",
    "end\n",
    "\n",
    "\n",
    "function specify_model_and_data(results_data, data_type, model_type)\n",
    "    length = nrow(results_data)\n",
    "\n",
    "    data_column = categorical(fill(data_type, length))\n",
    "    model_column = categorical(fill(model_type, length))\n",
    "\n",
    "    types_df = DataFrame(dataset=data_column, model=model_column)\n",
    "\n",
    "    return hcat(results_data, types_df)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating and storing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in n_points\n",
    "    results = compare_models(n)\n",
    "    CSV.write(results_path(n), results)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = Dict(n => CSV.read(results_path(n), DataFrame) for n in n_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_losses = DataFrame(\n",
    "    close_io=evaluate_on_flow_problem(forward_params, close_test_dataset, close_io_z_sols), \n",
    "    close_linreg=evaluate_on_flow_problem(forward_params, close_test_dataset, close_linreg_z_sols),\n",
    "    wide_io=evaluate_on_flow_problem(forward_params, wide_test_dataset, wide_io_z_sols),\n",
    "    wide_linreg=evaluate_on_flow_problem(forward_params, wide_test_dataset, wide_linreg_z_sols))\n",
    "\n",
    "first(task_losses, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe(task_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@df task_losses boxplot([:close_io, :close_linreg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@df task_losses boxplot([:wide_io, :wide_linreg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 5000\n",
    "n_above = df -> nrows(filter(row -> row > cutoff, df))\n",
    "\n",
    "println(\"Losses above $cutoff: $(n_above(task_losses.close_io))\")\n",
    "println(\"Losses above $cutoff: $(n_above(task_losses.close_linreg))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.0",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
