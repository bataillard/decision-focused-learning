
@video{institute_for_pure__applied_mathematics_ipam_tias_2023,
	title = {Tias Guns - Prediction + Optimisation: without and with decision-focused learning - {IPAM} at {UCLA}},
	url = {https://www.youtube.com/watch?v=Mauqt0Y1J2I},
	shorttitle = {Tias Guns - Prediction + Optimisation},
	abstract = {Recorded 02 March 2023. Tias Guns of {KU} Leuven presents "Prediction + Optimisation: without and with decision-focused learning" at {IPAM}'s Artificial Intelligence and Discrete Optimization Workshop.
Abstract: Industry and society are increasingly automating processes, which requires solving combinatorial optimisation problems. To find not just optimal solutions, but also 'desirable' solutions for the end user, it is increasingly important to offer {AI} tools that automatically learn from the user and the environment and that support the constraint modelling in interpretable ways.
In this talk I will provide an overview of three different ways in which {AI} can augment the modeling part of combinatorial optimisation. This includes learning from the user (preference learning in {VRP}), learning from the environment (end-to-end decision focussed learning) and explanation generation, that sit at the intersection of learning and reasoning. As part of this work, we are building a modern constraint programming language called {CPMpy}(http://cpmpy.readthedocs.io) that eases integration of multiple constraint solving paradigms with machine learning and other scientific python libraries. I will shortly highlight its possibilities beyond the above cases, as well as our larger vision of conversational human-aware technology for optimisation.
Learn more online at: http://www.ipam.ucla.edu/programs/wor...},
	author = {{Institute for Pure \& Applied Mathematics (IPAM)}},
	urldate = {2023-04-03},
	date = {2023-03-02},
	keywords = {unread, tutorial, video},
}

@collection{crainic_network_2021,
	location = {Cham},
	title = {Network Design with Applications to Transportation and Logistics},
	isbn = {978-3-030-64017-0},
	url = {https://link.springer.com/10.1007/978-3-030-64018-7},
	publisher = {Springer International Publishing},
	editor = {Crainic, Teodor Gabriel and Gendreau, Michel and Gendron, Bernard},
	urldate = {2023-04-04},
	date = {2021},
	langid = {english},
	doi = {10.1007/978-3-030-64018-7},
	keywords = {unread, City Logistics, Fixed-Cost Network Design, Logistics, Logistics Networks, Multi-Facility Network Design, Network Design, Rail Network, Transportation},
	file = {Full Text PDF:C\:\\Users\\Luca\\Zotero\\storage\\BVHGFQGQ\\Crainic et al. - 2021 - Network Design with Applications to Transportation.pdf:application/pdf},
}

@misc{paulus_comboptnet_2022,
	title = {{CombOptNet}: Fit the Right {NP}-Hard Problem by Learning Integer Programming Constraints},
	url = {http://arxiv.org/abs/2105.02343},
	doi = {10.48550/arXiv.2105.02343},
	shorttitle = {{CombOptNet}},
	abstract = {Bridging logical and algorithmic reasoning with modern machine learning techniques is a fundamental challenge with potentially transformative impact. On the algorithmic side, many {NP}-hard problems can be expressed as integer programs, in which the constraints play the role of their "combinatorial specification." In this work, we aim to integrate integer programming solvers into neural network architectures as layers capable of learning both the cost terms and the constraints. The resulting end-to-end trainable architectures jointly extract features from raw data and solve a suitable (learned) combinatorial problem with state-of-the-art integer programming solvers. We demonstrate the potential of such layers with an extensive performance analysis on synthetic data and with a demonstration on a competitive computer vision keypoint matching benchmark.},
	number = {{arXiv}:2105.02343},
	publisher = {{arXiv}},
	author = {Paulus, Anselm and Rolínek, Michal and Musil, Vít and Amos, Brandon and Martius, Georg},
	urldate = {2023-04-04},
	date = {2022-04-11},
	eprinttype = {arxiv},
	eprint = {2105.02343 [cs]},
	keywords = {Computer Science - Machine Learning, read, constraints, researchproposal},
	file = {arXiv Fulltext PDF:C\:\\Users\\Luca\\Zotero\\storage\\64NPU4CC\\Paulus et al. - 2022 - CombOptNet Fit the Right NP-Hard Problem by Learn.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Luca\\Zotero\\storage\\5YC4CS2K\\2105.html:text/html},
}

@misc{wilder_melding_2018,
	title = {Melding the Data-Decisions Pipeline: Decision-Focused Learning for Combinatorial Optimization},
	url = {http://arxiv.org/abs/1809.05504},
	doi = {10.48550/arXiv.1809.05504},
	shorttitle = {Melding the Data-Decisions Pipeline},
	abstract = {Creating impact in real-world settings requires artificial intelligence techniques to span the full pipeline from data, to predictive models, to decisions. These components are typically approached separately: a machine learning model is first trained via a measure of predictive accuracy, and then its predictions are used as input into an optimization algorithm which produces a decision. However, the loss function used to train the model may easily be misaligned with the end goal, which is to make the best decisions possible. Hand-tuning the loss function to align with optimization is a difficult and error-prone process (which is often skipped entirely). We focus on combinatorial optimization problems and introduce a general framework for decision-focused learning, where the machine learning model is directly trained in conjunction with the optimization algorithm to produce high-quality decisions. Technically, our contribution is a means of integrating common classes of discrete optimization problems into deep learning or other predictive models, which are typically trained via gradient descent. The main idea is to use a continuous relaxation of the discrete problem to propagate gradients through the optimization procedure. We instantiate this framework for two broad classes of combinatorial problems: linear programs and submodular maximization. Experimental results across a variety of domains show that decision-focused learning often leads to improved optimization performance compared to traditional methods. We find that standard measures of accuracy are not a reliable proxy for a predictive model's utility in optimization, and our method's ability to specify the true goal as the model's training objective yields substantial dividends across a range of decision problems.},
	number = {{arXiv}:1809.05504},
	publisher = {{arXiv}},
	author = {Wilder, Bryan and Dilkina, Bistra and Tambe, Milind},
	urldate = {2023-04-04},
	date = {2018-11-20},
	eprinttype = {arxiv},
	eprint = {1809.05504 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning, read, researchproposal},
	file = {arXiv Fulltext PDF:C\:\\Users\\Luca\\Zotero\\storage\\I3YLCMWP\\Wilder et al. - 2018 - Melding the Data-Decisions Pipeline Decision-Focu.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Luca\\Zotero\\storage\\NRJ22EKR\\1809.html:text/html},
}

@article{ferber_mipaal_2020,
	title = {{MIPaaL}: Mixed Integer Program as a Layer},
	volume = {34},
	rights = {Copyright (c) 2020 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/5509},
	doi = {10.1609/aaai.v34i02.5509},
	shorttitle = {{MIPaaL}},
	abstract = {Machine learning components commonly appear in larger decision-making pipelines; however, the model training process typically focuses only on a loss that measures average accuracy between predicted values and ground truth values. Decision-focused learning explicitly integrates the downstream decision problem when training the predictive model, in order to optimize the quality of decisions induced by the predictions. It has been successfully applied to several limited combinatorial problem classes, such as those that can be expressed as linear programs ({LP}), and submodular optimization. However, these previous applications have uniformly focused on problems with simple constraints. Here, we enable decision-focused learning for the broad class of problems that can be encoded as a mixed integer linear program ({MIP}), hence supporting arbitrary linear constraints over discrete and continuous variables. We show how to differentiate through a {MIP} by employing a cutting planes solution approach, an algorithm that iteratively tightens the continuous relaxation by adding constraints removing fractional solutions. We evaluate our new end-to-end approach on several real world domains and show that it outperforms the standard two phase approaches that treat prediction and optimization separately, as well as a baseline approach of simply applying decision-focused learning to the {LP} relaxation of the {MIP}. Lastly, we demonstrate generalization performance in several transfer learning tasks.},
	pages = {1504--1511},
	number = {2},
	journaltitle = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
	author = {Ferber, Aaron and Wilder, Bryan and Dilkina, Bistra and Tambe, Milind},
	urldate = {2023-04-04},
	date = {2020-04-03},
	langid = {english},
	note = {Number: 02},
	keywords = {read, researchproposal},
	file = {Full Text PDF:C\:\\Users\\Luca\\Zotero\\storage\\V6RJLSBJ\\Ferber et al. - 2020 - MIPaaL Mixed Integer Program as a Layer.pdf:application/pdf},
}

@misc{mandi_smart_2019,
	title = {Smart Predict-and-Optimize for Hard Combinatorial Optimization Problems},
	url = {http://arxiv.org/abs/1911.10092},
	doi = {10.48550/arXiv.1911.10092},
	abstract = {Combinatorial optimization assumes that all parameters of the optimization problem, e.g. the weights in the objective function is fixed. Often, these weights are mere estimates and increasingly machine learning techniques are used to for their estimation. Recently, Smart Predict and Optimize ({SPO}) has been proposed for problems with a linear objective function over the predictions, more specifically linear programming problems. It takes the regret of the predictions on the linear problem into account, by repeatedly solving it during learning. We investigate the use of {SPO} to solve more realistic discrete optimization problems. The main challenge is the repeated solving of the optimization problem. To this end, we investigate ways to relax the problem as well as warmstarting the learning and the solving. Our results show that even for discrete problems it often suffices to train by solving the relaxation in the {SPO} loss. Furthermore, this approach outperforms, for most instances, the state-of-the-art approach of Wilder, Dilkina, and Tambe. We experiment with weighted knapsack problems as well as complex scheduling problems and show for the first time that a predict-and-optimize approach can successfully be used on large-scale combinatorial optimization problems.},
	number = {{arXiv}:1911.10092},
	publisher = {{arXiv}},
	author = {Mandi, Jaynta and Demirović, Emir and Stuckey, Peter J. and Guns, Tias},
	urldate = {2023-04-04},
	date = {2019-11-22},
	eprinttype = {arxiv},
	eprint = {1911.10092 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Computer Science - Artificial Intelligence, unread, researchproposal},
	file = {arXiv Fulltext PDF:C\:\\Users\\Luca\\Zotero\\storage\\NWYH8MP9\\Mandi et al. - 2019 - Smart Predict-and-Optimize for Hard Combinatorial .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Luca\\Zotero\\storage\\G2G3EXAL\\1911.html:text/html},
}

@misc{mulamba_discrete_2020,
	title = {Discrete solution pools and noise-contrastive estimation for predict-and-optimize},
	abstract = {Numerous real-life decision-making processes involve solving a combinatorial optimization problem with uncertain input that can be estimated from historic data. There is a growing interest in decision-focused learning methods, where the loss function used for learning to predict the uncertain input uses the outcome of solving the combinatorial problem over a set of predictions. Different surrogate loss functions have been identified, often using a continuous approximation of the combinatorial problem. However, a key bottleneck is that to compute the loss, one has to solve the combinatorial optimisation problem for each training instance in each epoch, which is computationally expensive even in the case of continuous approximations. We propose a different solver-agnostic method for decision-focused learning, namely by considering a pool of feasible solutions as a discrete approximation of the full combinatorial problem. Solving is now trivial through a single pass over the solution pool. We design several variants of a noise-contrastive loss over the solution pool, which we substantiate theoretically and empirically. Furthermore, we show that by dynamically re-solving only a fraction of the training instances each epoch, our method performs on par with the state of the art, whilst drastically reducing the time spent solving, hence increasing the feasibility of predict-and-optimize for larger problems.},
	author = {Mulamba, Maxime and Mandi, Jayanta and Diligenti, Michelangelo and Lombardi, Michele and Bucarey, Víctor and Guns, Tias},
	date = {2020-11-10},
	keywords = {read, researchproposal},
	file = {Full Text PDF:C\:\\Users\\Luca\\Zotero\\storage\\NBZX9WG3\\Mulamba et al. - 2020 - Discrete solution pools and noise-contrastive esti.pdf:application/pdf},
}

@inproceedings{mandi_interior_2020,
	title = {Interior Point Solving for {LP}-based prediction+optimisation},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/51311013e51adebc3c34d2cc591fefee-Abstract.html},
	abstract = {Solving optimization problem is the key to decision making in many real-life analytics applications. However, the coefficients of the optimization problems are often uncertain and dependent on external factors, such as future demand or energy- or stock prices. Machine learning ({ML}) models, especially neural networks, are increasingly being used to estimate these coefficients in a data-driven way. Hence, end-to-end predict-and-optimize approaches, which consider how effective the predicted values are to solve the optimization problem, have received increasing attention. In case of integer linear programming problems, a popular approach to overcome their non-differentiabilty is to add a quadratic penalty term to the continuous relaxation, such that results from differentiating over quadratic programs can be used. Instead we investigate the use of the more principled logarithmic barrier term, as widely used in interior point solvers for linear programming. Instead of differentiating the {KKT} conditions, we consider the homogeneous self-dual formulation of the {LP} and we show the relation between the interior point step direction and corresponding gradients needed for learning. Finally, our empirical experiments demonstrate our approach performs as good as if not better than the state-of-the-art {QPTL} (Quadratic Programming task loss) formulation of Wilder et al. and {SPO} approach of Elmachtoub and Grigas.},
	pages = {7272--7282},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Mandi, Jayanta and Guns, Tias},
	urldate = {2023-04-04},
	date = {2020},
	keywords = {read, researchproposal},
	file = {Full Text PDF:C\:\\Users\\Luca\\Zotero\\storage\\S8Y3GYYG\\Mandi and Guns - 2020 - Interior Point Solving for LP-based prediction+opt.pdf:application/pdf},
}

@misc{donti_task-based_2019,
	title = {Task-based End-to-end Model Learning in Stochastic Optimization},
	url = {http://arxiv.org/abs/1703.04529},
	doi = {10.48550/arXiv.1703.04529},
	abstract = {With the increasing popularity of machine learning techniques, it has become common to see prediction algorithms operating within some larger process. However, the criteria by which we train these algorithms often differ from the ultimate criteria on which we evaluate them. This paper proposes an end-to-end approach for learning probabilistic machine learning models in a manner that directly captures the ultimate task-based objective for which they will be used, within the context of stochastic programming. We present three experimental evaluations of the proposed approach: a classical inventory stock problem, a real-world electrical grid scheduling task, and a real-world energy storage arbitrage task. We show that the proposed approach can outperform both traditional modeling and purely black-box policy optimization approaches in these applications.},
	number = {{arXiv}:1703.04529},
	publisher = {{arXiv}},
	author = {Donti, Priya L. and Amos, Brandon and Kolter, J. Zico},
	urldate = {2023-04-04},
	date = {2019-04-25},
	eprinttype = {arxiv},
	eprint = {1703.04529 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, unread, researchproposal},
	file = {arXiv Fulltext PDF:C\:\\Users\\Luca\\Zotero\\storage\\IFUT6MY4\\Donti et al. - 2019 - Task-based End-to-end Model Learning in Stochastic.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Luca\\Zotero\\storage\\753SZGYY\\1703.html:text/html},
}

@article{elmachtoub_smart_2022,
	title = {Smart “Predict, then Optimize”},
	volume = {68},
	issn = {0025-1909},
	url = {https://pubsonline.informs.org/doi/abs/10.1287/mnsc.2020.3922},
	doi = {10.1287/mnsc.2020.3922},
	abstract = {Many real-world analytics problems involve two significant challenges: prediction and optimization. Because of the typically complex nature of each challenge, the standard paradigm is predict-then-optimize. By and large, machine learning tools are intended to minimize prediction error and do not account for how the predictions will be used in the downstream optimization problem. In contrast, we propose a new and very general framework, called Smart “Predict, then Optimize” ({SPO}), which directly leverages the optimization problem structure—that is, its objective and constraints—for designing better prediction models. A key component of our framework is the {SPO} loss function, which measures the decision error induced by a prediction. Training a prediction model with respect to the {SPO} loss is computationally challenging, and, thus, we derive, using duality theory, a convex surrogate loss function, which we call the {SPO}+ loss. Most importantly, we prove that the {SPO}+ loss is statistically consistent with respect to the {SPO} loss under mild conditions. Our {SPO}+ loss function can tractably handle any polyhedral, convex, or even mixed-integer optimization problem with a linear objective. Numerical experiments on shortest-path and portfolio-optimization problems show that the {SPO} framework can lead to significant improvement under the predict-then-optimize paradigm, in particular, when the prediction model being trained is misspecified. We find that linear models trained using {SPO}+ loss tend to dominate random-forest algorithms, even when the ground truth is highly nonlinear.

This paper was accepted by Yinyu Ye, optimization.

Supplemental Material: Data and the online appendix are available at https://doi.org/10.1287/mnsc.2020.3922},
	pages = {9--26},
	number = {1},
	journaltitle = {Management Science},
	author = {Elmachtoub, Adam N. and Grigas, Paul},
	urldate = {2023-04-04},
	date = {2022-01},
	note = {Publisher: {INFORMS}},
	keywords = {data-driven optimization, linear regression, machine learning, prescriptive analytics, read},
	file = {Full Text PDF:C\:\\Users\\Luca\\Zotero\\storage\\99GGTQN9\\Elmachtoub and Grigas - 2022 - Smart “Predict, then Optimize”.pdf:application/pdf},
}

@misc{chan_inverse_2022,
	title = {Inverse Optimization: Theory and Applications},
	url = {http://arxiv.org/abs/2109.03920},
	doi = {10.48550/arXiv.2109.03920},
	shorttitle = {Inverse Optimization},
	abstract = {Inverse optimization describes a process that is the "reverse" of traditional mathematical optimization. Unlike traditional optimization, which seeks to compute optimal decisions given an objective and constraints, inverse optimization takes decisions as input and determines an objective and/or constraints that render these decisions approximately or exactly optimal. In recent years, there has been an explosion of interest in the mathematics and applications of inverse optimization. This paper provides a comprehensive review of both the methodological and application-oriented literature in inverse optimization.},
	number = {{arXiv}:2109.03920},
	publisher = {{arXiv}},
	author = {Chan, Timothy C. Y. and Mahmood, Rafid and Zhu, Ian Yihang},
	urldate = {2023-04-04},
	date = {2022-07-27},
	eprinttype = {arxiv},
	eprint = {2109.03920 [math]},
	keywords = {Mathematics - Optimization and Control, lessrel, invopt, read, researchproposal-lessrel},
	file = {arXiv Fulltext PDF:C\:\\Users\\Luca\\Zotero\\storage\\7XM88GQW\\Chan et al. - 2022 - Inverse Optimization Theory and Applications.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Luca\\Zotero\\storage\\ZKJYJJKH\\2109.html:text/html},
}

@inproceedings{kotary_2021_endtoend,
  title     = {End-to-End Constrained Optimization Learning: A Survey},
  author    = {Kotary, James and Fioretto, Ferdinando and Van Hentenryck, Pascal and Wilder, Bryan},
  booktitle = {Proceedings of the Thirtieth International Joint Conference on
               Artificial Intelligence, {IJCAI-21}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  editor    = {Zhi-Hua Zhou},
  pages     = {4475--4482},
  year      = {2021},
  month     = {8},
  note      = {Survey Track},
  doi       = {10.24963/ijcai.2021/610},
  url       = {https://doi.org/10.24963/ijcai.2021/610},
}


@misc{laage_2021_twostep,
      title={A Two-step Heuristic for the Periodic Demand Estimation Problem}, 
      author={Greta Laage and Emma Frejinger and Gilles Savard},
      year={2021},
      eprint={2108.08331},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}

@misc{laage_2022_periodic,
      title={Periodic Freight Demand Estimation for Large-scale Tactical Planning}, 
      author={Greta Laage and Emma Frejinger and Gilles Savard},
      year={2022},
      eprint={2105.09136},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{bodur_inverse_2021,
	title = {Inverse Mixed Integer Optimization: Polyhedral Insights and Trust Region Methods},
	url = {http://arxiv.org/abs/2008.00301},
	doi = {10.48550/arXiv.2008.00301},
	shorttitle = {Inverse Mixed Integer Optimization},
	abstract = {Inverse optimization, determining parameters of an optimization problem that render a given solution optimal, has received increasing attention in recent years. While significant inverse optimization literature exists for convex optimization problems, there have been few advances for discrete problems, despite the ubiquity of applications that fundamentally rely on discrete decision-making. In this paper, we present a new set of theoretical insights and algorithms for the general class of inverse mixed integer linear optimization problems. Specifically, a general characterization of optimality conditions is established and leveraged to design new cutting plane solution algorithms. Through an extensive set of computational experiments, we show that our methods provide substantial improvements over existing methods in solving the largest and most difficult instances to date.},
	number = {{arXiv}:2008.00301},
	publisher = {{arXiv}},
	author = {Bodur, Merve and Chan, Timothy C. Y. and Zhu, Ian Yihang},
	urldate = {2023-04-04},
	date = {2021-08-31},
	eprinttype = {arxiv},
	eprint = {2008.00301 [math]},
	keywords = {Mathematics - Optimization and Control, lessrel, invopt, read, researchproposal-lessrel},
	file = {arXiv Fulltext PDF:C\:\\Users\\Luca\\Zotero\\storage\\JJCDHYQ4\\Bodur et al. - 2021 - Inverse Mixed Integer Optimization Polyhedral Ins.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Luca\\Zotero\\storage\\I64USBS3\\2008.html:text/html},
}

@article{chan_inverse_2020,
	title = {Inverse optimization for the recovery of constraint parameters},
	volume = {282},
	issn = {0377-2217},
	url = {https://www.sciencedirect.com/science/article/pii/S0377221719307830},
	doi = {10.1016/j.ejor.2019.09.027},
	abstract = {Most inverse optimization models impute unspecified parameters of an objective function to make an observed solution optimal for a given optimization problem with a fixed feasible set. We propose two approaches to impute unspecified left-hand-side constraint coefficients in addition to a cost vector for a given linear optimization problem. The first approach identifies parameters minimizing the duality gap, while the second minimally perturbs prior estimates of the unspecified parameters to satisfy strong duality, if it is possible to satisfy the optimality conditions exactly. We apply these two approaches to the general linear optimization problem. We also use them to impute unspecified parameters of the uncertainty set for robust linear optimization problems under interval and cardinality constrained uncertainty. Each inverse optimization model we propose is nonconvex, but we show that a globally optimal solution can be obtained either in closed form or by solving a linear number of linear or convex optimization problems.},
	pages = {415--427},
	number = {2},
	journaltitle = {European Journal of Operational Research},
	shortjournal = {European Journal of Operational Research},
	author = {Chan, Timothy C. Y. and Kaw, Neal},
	urldate = {2023-04-04},
	date = {2020-04-16},
	langid = {english},
	keywords = {lessrel, invopt, Inverse optimization, Linear programming, Parameter estimation, Robust optimization, read, researchproposal-lessrel},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\Luca\\Zotero\\storage\\D87LJCWU\\Chan and Kaw - 2020 - Inverse optimization for the recovery of constrain.pdf:application/pdf},
}

@article{ghobadi_inferring_2021,
	title = {Inferring linear feasible regions using inverse optimization},
	volume = {290},
	issn = {0377-2217},
	url = {https://www.sciencedirect.com/science/article/pii/S037722172030761X},
	doi = {10.1016/j.ejor.2020.08.048},
	abstract = {Consider a problem where a set of feasible observations are provided by an expert, and a cost function exists that characterizes which of the observations dominate the others and are hence, preferred. Assume the expert has an implicit optimization model in mind to identify the feasible observations, but the explicit constraints of this underlying model are unknown. Our goal is to infer the feasible region of such an optimization model that would render these observations feasible while making the best ones optimal for the cost (objective) function. Such feasible regions (i) build a baseline for a systematic categorization of future observations, and (ii) allow for using sensitivity analysis to discern changes in optimal solutions if the objective function changes in the future. In this paper, we propose a general inverse optimization methodology that recovers the complete constraint matrix of a linear model and then introduce a tractable equivalent reformulation. Furthermore, we provide and discuss several generalized loss functions to inform the desirable properties of the feasible region based on user preference and historical data. We demonstrate our approach using numerical examples and a realistic diet recommendation case study for imputing personalized diets. Our numerical examples verify the validity of our approach and emphasize the differences among the proposed loss functions. The diet recommendation case study shows that the proposed models can improve the palatability of the recommended diets for each user, and it provides further intuition for large-scale implementations of the proposed methodology.},
	pages = {829--843},
	number = {3},
	journaltitle = {European Journal of Operational Research},
	shortjournal = {European Journal of Operational Research},
	author = {Ghobadi, Kimia and Mahmoudzadeh, Houra},
	urldate = {2023-04-04},
	date = {2021-05-01},
	langid = {english},
	keywords = {lessrel, invopt, Inverse optimization, Linear programming, Diet recommendation, Feasible region inference, Loss function, read, researchproposal-lessrel},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\Luca\\Zotero\\storage\\ZMYP97VM\\Ghobadi and Mahmoudzadeh - 2021 - Inferring linear feasible regions using inverse op.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Luca\\Zotero\\storage\\CY2UIQQP\\S037722172030761X.html:text/html},
}

@misc{dalle_learning_2022,
	title = {Learning with Combinatorial Optimization Layers: a Probabilistic Approach},
	url = {http://arxiv.org/abs/2207.13513},
	doi = {10.48550/arXiv.2207.13513},
	shorttitle = {Learning with Combinatorial Optimization Layers},
	abstract = {Combinatorial optimization ({CO}) layers in machine learning ({ML}) pipelines are a powerful tool to tackle data-driven decision tasks, but they come with two main challenges. First, the solution of a {CO} problem often behaves as a piecewise constant function of its objective parameters. Given that {ML} pipelines are typically trained using stochastic gradient descent, the absence of slope information is very detrimental. Second, standard {ML} losses do not work well in combinatorial settings. A growing body of research addresses these challenges through diverse methods. Unfortunately, the lack of well-maintained implementations slows down the adoption of {CO} layers. In this paper, building upon previous works, we introduce a probabilistic perspective on {CO} layers, which lends itself naturally to approximate differentiation and the construction of structured losses. We recover many approaches from the literature as special cases, and we also derive new ones. Based on this unifying perspective, we present {InferOpt}.jl, an open-source Julia package that 1) allows turning any {CO} oracle with a linear objective into a differentiable layer, and 2) defines adequate losses to train pipelines containing such layers. Our library works with arbitrary optimization algorithms, and it is fully compatible with Julia's {ML} ecosystem. We demonstrate its abilities using a pathfinding problem on video game maps as guiding example, as well as three other applications from operations research.},
	number = {{arXiv}:2207.13513},
	publisher = {{arXiv}},
	author = {Dalle, Guillaume and Baty, Léo and Bouvier, Louis and Parmentier, Axel},
	urldate = {2023-04-06},
	date = {2022-12-03},
	eprinttype = {arxiv},
	eprint = {2207.13513 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning, unread, promising},
	file = {arXiv Fulltext PDF:C\:\\Users\\Luca\\Zotero\\storage\\NKU3C7TH\\Dalle et al. - 2022 - Learning with Combinatorial Optimization Layers a.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Luca\\Zotero\\storage\\QPD6JSYF\\2207.html:text/html},
}

@misc{teso_machine_2022,
	title = {Machine Learning for Combinatorial Optimisation of Partially-Specified Problems: Regret Minimisation as a Unifying Lens},
	url = {http://arxiv.org/abs/2205.10157},
	doi = {10.48550/arXiv.2205.10157},
	shorttitle = {Machine Learning for Combinatorial Optimisation of Partially-Specified Problems},
	abstract = {It is increasingly common to solve combinatorial optimisation problems that are partially-specified. We survey the case where the objective function or the relations between variables are not known or are only partially specified. The challenge is to learn them from available data, while taking into account a set of hard constraints that a solution must satisfy, and that solving the optimisation problem (esp. during learning) is computationally very demanding. This paper overviews four seemingly unrelated approaches, that can each be viewed as learning the objective function of a hard combinatorial optimisation problem: 1) surrogate-based optimisation, 2) empirical model learning, 3) decision-focused learning (`predict + optimise'), and 4) structured-output prediction. We formalise each learning paradigm, at first in the ways commonly found in the literature, and then bring the formalisations together in a compatible way using regret. We discuss the differences and interactions between these frameworks, highlight the opportunities for cross-fertilization and survey open directions.},
	number = {{arXiv}:2205.10157},
	publisher = {{arXiv}},
	author = {Teso, Stefano and Bliek, Laurens and Borghesi, Andrea and Lombardi, Michele and Yorke-Smith, Neil and Guns, Tias and Passerini, Andrea},
	urldate = {2023-04-06},
	date = {2022-05-20},
	eprinttype = {arxiv},
	eprint = {2205.10157 [cs]},
	keywords = {Computer Science - Machine Learning, unread, promising},
	file = {arXiv Fulltext PDF:C\:\\Users\\Luca\\Zotero\\storage\\H6VLBL5V\\Teso et al. - 2022 - Machine Learning for Combinatorial Optimisation of.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Luca\\Zotero\\storage\\YJQA6M22\\2205.html:text/html},
}

@misc{nandwani_solver-free_2023,
	title = {A Solver-Free Framework for Scalable Learning in Neural {ILP} Architectures},
	url = {http://arxiv.org/abs/2210.09082},
	doi = {10.48550/arXiv.2210.09082},
	abstract = {There is a recent focus on designing architectures that have an Integer Linear Programming ({ILP}) layer within a neural model (referred to as Neural {ILP} in this paper). Neural {ILP} architectures are suitable for pure reasoning tasks that require data-driven constraint learning or for tasks requiring both perception (neural) and reasoning ({ILP}). A recent {SOTA} approach for end-to-end training of Neural {ILP} explicitly defines gradients through the {ILP} black box (Paulus et al. 2021) - this trains extremely slowly, owing to a call to the underlying {ILP} solver for every training data point in a minibatch. In response, we present an alternative training strategy that is solver-free, i.e., does not call the {ILP} solver at all at training time. Neural {ILP} has a set of trainable hyperplanes (for cost and constraints in {ILP}), together representing a polyhedron. Our key idea is that the training loss should impose that the final polyhedron separates the positives (all constraints satisfied) from the negatives (at least one violated constraint or a suboptimal cost value), via a soft-margin formulation. While positive example(s) are provided as part of the training data, we devise novel techniques for generating negative samples. Our solution is flexible enough to handle equality as well as inequality constraints. Experiments on several problems, both perceptual as well as symbolic, which require learning the constraints of an {ILP}, show that our approach has superior performance and scales much better compared to purely neural baselines and other state-of-the-art models that require solver-based training. In particular, we are able to obtain excellent performance in 9 x 9 symbolic and visual sudoku, to which the other Neural {ILP} solver is not able to scale.},
	number = {{arXiv}:2210.09082},
	publisher = {{arXiv}},
	author = {Nandwani, Yatin and Ranjan, Rishabh and Mausam and Singla, Parag},
	urldate = {2023-04-06},
	date = {2023-01-13},
	eprinttype = {arxiv},
	eprint = {2210.09082 [cs]},
	keywords = {Computer Science - Machine Learning, unread, promising},
	file = {arXiv Fulltext PDF:C\:\\Users\\Luca\\Zotero\\storage\\A2EAVWJE\\Nandwani et al. - 2023 - A Solver-Free Framework for Scalable Learning in N.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Luca\\Zotero\\storage\\F6MFQR2E\\2210.html:text/html},
}

@misc{kotary_folded_2023,
	title = {Folded Optimization for End-to-End Model-Based Learning},
	url = {http://arxiv.org/abs/2301.12047},
	doi = {10.48550/arXiv.2301.12047},
	abstract = {The integration of constrained optimization models as components in deep networks has led to promising advances in both these domains. A primary challenge in this setting is backpropagation through the optimization mapping, which typically lacks a closed form. A common approach is unrolling, which relies on automatic differentiation through the operations of an iterative solver. While flexible and general, unrolling can encounter accuracy and efficiency issues in practice. These issues can be avoided by differentiating the optimization mapping analytically, but current frameworks impose rigid requirements on the optimization problem's form. This paper provides theoretical insights into the backpropagation of unrolled optimizers, which lead to a system for generating equivalent but efficiently solvable analytical models. Additionally, it proposes a unifying view of unrolling and analytical differentiation through constrained optimization mappings. Experiments over various structured prediction and decision-focused learning tasks illustrate the potential of the approach both computationally and in terms of enhanced expressiveness.},
	number = {{arXiv}:2301.12047},
	publisher = {{arXiv}},
	author = {Kotary, James and Dinh, My H. and Fioretto, Ferdinando},
	urldate = {2023-04-06},
	date = {2023-01-27},
	eprinttype = {arxiv},
	eprint = {2301.12047 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, unread, promising},
	file = {arXiv Fulltext PDF:C\:\\Users\\Luca\\Zotero\\storage\\5XXKTYGF\\Kotary et al. - 2023 - Folded Optimization for End-to-End Model-Based Lea.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Luca\\Zotero\\storage\\L5KPPYDU\\2301.html:text/html},
}

@misc{yan_surrogate_2021,
	title = {A Surrogate Objective Framework for Prediction+Optimization with Soft Constraints},
	url = {http://arxiv.org/abs/2111.11358},
	doi = {10.48550/arXiv.2111.11358},
	abstract = {Prediction+optimization is a common real-world paradigm where we have to predict problem parameters before solving the optimization problem. However, the criteria by which the prediction model is trained are often inconsistent with the goal of the downstream optimization problem. Recently, decision-focused prediction approaches, such as {SPO}+ and direct optimization, have been proposed to fill this gap. However, they cannot directly handle the soft constraints with the \$max\$ operator required in many real-world objectives. This paper proposes a novel analytically differentiable surrogate objective framework for real-world linear and semi-definite negative quadratic programming problems with soft linear and non-negative hard constraints. This framework gives the theoretical bounds on constraints' multipliers, and derives the closed-form solution with respect to predictive parameters and thus gradient for any variable in the problem. We evaluate our method in three applications extended with soft constraints: synthetic linear programming, portfolio optimization, and resource provisioning, demonstrating that our method outperforms traditional two-staged methods and other decision-focused approaches.},
	number = {{arXiv}:2111.11358},
	publisher = {{arXiv}},
	author = {Yan, Kai and Yan, Jie and Luo, Chuan and Chen, Liting and Lin, Qingwei and Zhang, Dongmei},
	urldate = {2023-04-06},
	date = {2021-11-22},
	eprinttype = {arxiv},
	eprint = {2111.11358 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Computer Science - Artificial Intelligence, unread, promising},
	file = {arXiv Fulltext PDF:C\:\\Users\\Luca\\Zotero\\storage\\Q4RN8R8K\\Yan et al. - 2021 - A Surrogate Objective Framework for Prediction+Opt.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Luca\\Zotero\\storage\\FNT27ZNE\\2111.html:text/html},
}

@misc{elmachtoub_decision_2020,
	title = {Decision Trees for Decision-Making under the Predict-then-Optimize Framework},
	url = {http://arxiv.org/abs/2003.00360},
	doi = {10.48550/arXiv.2003.00360},
	abstract = {We consider the use of decision trees for decision-making problems under the predict-then-optimize framework. That is, we would like to first use a decision tree to predict unknown input parameters of an optimization problem, and then make decisions by solving the optimization problem using the predicted parameters. A natural loss function in this framework is to measure the suboptimality of the decisions induced by the predicted input parameters, as opposed to measuring loss using input parameter prediction error. This natural loss function is known in the literature as the Smart Predict-then-Optimize ({SPO}) loss, and we propose a tractable methodology called {SPO} Trees ({SPOTs}) for training decision trees under this loss. {SPOTs} benefit from the interpretability of decision trees, providing an interpretable segmentation of contextual features into groups with distinct optimal solutions to the optimization problem of interest. We conduct several numerical experiments on synthetic and real data including the prediction of travel times for shortest path problems and predicting click probabilities for news article recommendation. We demonstrate on these datasets that {SPOTs} simultaneously provide higher quality decisions and significantly lower model complexity than other machine learning approaches (e.g., {CART}) trained to minimize prediction error.},
	number = {{arXiv}:2003.00360},
	publisher = {{arXiv}},
	author = {Elmachtoub, Adam N. and Liang, Jason Cheuk Nam and {McNellis}, Ryan},
	urldate = {2023-04-11},
	date = {2020-06-17},
	eprinttype = {arxiv},
	eprint = {2003.00360 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning, unread, decisiontrees},
	file = {arXiv Fulltext PDF:C\:\\Users\\Luca\\Zotero\\storage\\VRHY64TZ\\Elmachtoub et al. - 2020 - Decision Trees for Decision-Making under the Predi.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Luca\\Zotero\\storage\\2S6F5JB6\\2003.html:text/html},
}

@inproceedings{pogancic_differentiation_2020,
	title = {Differentiation of Blackbox Combinatorial Solvers},
	url = {https://openreview.net/forum?id=BkevoJSYPB},
	abstract = {Achieving fusion of deep learning with combinatorial algorithms promises transformative changes to artificial intelligence. One possible approach is to introduce combinatorial building blocks into neural networks. Such end-to-end architectures have the potential to tackle combinatorial problems on raw input data such as ensuring global consistency in multi-object tracking or route planning on maps in robotics. In this work, we present a method that implements an efficient backward pass through blackbox implementations of combinatorial solvers with linear objective functions. We provide both theoretical and experimental backing. In particular, we incorporate the Gurobi {MIP} solver, Blossom V algorithm, and Dijkstra's algorithm into architectures that extract suitable features from raw inputs for the traveling salesman problem, the min-cost perfect matching problem and the shortest path problem.},
	eventtitle = {International Conference on Learning Representations},
	author = {Pogančić, Marin Vlastelica and Paulus, Anselm and Musil, Vit and Martius, Georg and Rolinek, Michal},
	urldate = {2023-04-11},
	date = {2020-03-11},
	langid = {english},
	keywords = {read, researchproposal},
	file = {Full Text PDF:C\:\\Users\\Luca\\Zotero\\storage\\ALHS9T4I\\Pogančić et al. - 2020 - Differentiation of Blackbox Combinatorial Solvers.pdf:application/pdf},
}

@inproceedings{tan_learning_2020,
	title = {Learning Linear Programs from Optimal Decisions},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/e44e875c12109e4fa3716c05008048b2-Abstract.html},
	abstract = {We propose a flexible gradient-based framework for learning linear programs from optimal decisions. Linear programs are often specified by hand, using prior knowledge of relevant costs and constraints. In some applications, linear programs must instead be learned from observations of optimal decisions. Learning from optimal decisions is a particularly challenging bilevel problem, and much of the related inverse optimization literature is dedicated to special cases. We tackle the general problem, learning all parameters jointly while allowing flexible parameterizations of costs, constraints, and loss functions. We also address challenges specific to learning linear programs, such as empty feasible regions and non-unique optimal decisions. Experiments show that our method successfully learns synthetic linear programs and minimum-cost multi-commodity flow instances for which previous methods are not directly applicable. We also provide a fast batch-mode {PyTorch} implementation of the homogeneous interior point algorithm, which supports gradients by implicit differentiation or backpropagation.},
	pages = {19738--19749},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Tan, Yingcong and Terekhov, Daria and Delong, Andrew},
	urldate = {2023-04-12},
	date = {2020},
	keywords = {unread, promising},
	file = {Full Text PDF:C\:\\Users\\Luca\\Zotero\\storage\\MG7JSTZQ\\Tan et al. - 2020 - Learning Linear Programs from Optimal Decisions.pdf:application/pdf},
}

@misc{agrawal_differentiable_2019,
	title = {Differentiable Convex Optimization Layers},
	url = {http://arxiv.org/abs/1910.12430},
	doi = {10.48550/arXiv.1910.12430},
	abstract = {Recent work has shown how to embed differentiable optimization problems (that is, problems whose solutions can be backpropagated through) as layers within deep learning architectures. This method provides a useful inductive bias for certain problems, but existing software for differentiable optimization layers is rigid and difficult to apply to new settings. In this paper, we propose an approach to differentiating through disciplined convex programs, a subclass of convex optimization problems used by domain-specific languages ({DSLs}) for convex optimization. We introduce disciplined parametrized programming, a subset of disciplined convex programming, and we show that every disciplined parametrized program can be represented as the composition of an affine map from parameters to problem data, a solver, and an affine map from the solver's solution to a solution of the original problem (a new form we refer to as affine-solver-affine form). We then demonstrate how to efficiently differentiate through each of these components, allowing for end-to-end analytical differentiation through the entire convex program. We implement our methodology in version 1.1 of {CVXPY}, a popular Python-embedded {DSL} for convex optimization, and additionally implement differentiable layers for disciplined convex programs in {PyTorch} and {TensorFlow} 2.0. Our implementation significantly lowers the barrier to using convex optimization problems in differentiable programs. We present applications in linear machine learning models and in stochastic control, and we show that our layer is competitive (in execution time) compared to specialized differentiable solvers from past work.},
	number = {{arXiv}:1910.12430},
	publisher = {{arXiv}},
	author = {Agrawal, Akshay and Amos, Brandon and Barratt, Shane and Boyd, Stephen and Diamond, Steven and Kolter, Zico},
	urldate = {2023-04-26},
	date = {2019-10-28},
	eprinttype = {arxiv},
	eprint = {1910.12430 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning, unread, comboptnet},
	file = {arXiv Fulltext PDF:C\:\\Users\\Luca\\Zotero\\storage\\LEKTDQ8Z\\Agrawal et al. - 2019 - Differentiable Convex Optimization Layers.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Luca\\Zotero\\storage\\HSYIICXW\\1910.html:text/html},
}
