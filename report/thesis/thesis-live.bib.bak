
@book{crainic_network_2021,
	address = {Cham},
	title = {Network {Design} with {Applications} to {Transportation} and {Logistics}},
	isbn = {978-3-030-64017-0},
	url = {https://link.springer.com/10.1007/978-3-030-64018-7},
	language = {en},
	urldate = {2023-04-04},
	publisher = {Springer International Publishing},
	author = {Crainic, Teodor Gabriel and Gendreau, Michel and Gendron, Bernard},
	year = {2021},
	doi = {10.1007/978-3-030-64018-7},
	keywords = {City Logistics, Fixed-Cost Network Design, Logistics, Logistics Networks, Multi-Facility Network Design, Network Design, Rail Network, Transportation, unread},
}

@book{karush_minima_1939,
	title = {Minima of functions of several variables with inequalities as side conditions},
	url = {https://catalog.lib.uchicago.edu/vufind/Record/4111654},
	urldate = {2023-09-12},
	author = {Karush, William},
	year = {1939},
	note = {OCLC: 43268508},
	keywords = {Functions},
}

@incollection{kuhn_nonlinear_1951,
	title = {Nonlinear {Programming}},
	volume = {2},
	url = {https://projecteuclid.org/ebooks/berkeley-symposium-on-mathematical-statistics-and-probability/Proceedings-of-the-Second-Berkeley-Symposium-on-Mathematical-Statistics-and/chapter/Nonlinear-Programming/bsmsp/1200500249},
	urldate = {2023-09-12},
	booktitle = {Proceedings of the {Second} {Berkeley} {Symposium} on {Mathematical} {Statistics} and {Probability}},
	publisher = {University of California Press},
	author = {Kuhn, H. W. and Tucker, A. W.},
	month = jan,
	year = {1951},
	pages = {481--493},
}

@article{cadarso_strategic_2018,
	title = {On strategic multistage operational two-stage stochastic 0–1 optimization for the {Rapid} {Transit} {Network} {Design} problem},
	volume = {271},
	issn = {0377-2217},
	url = {https://www.sciencedirect.com/science/article/pii/S0377221718304521},
	doi = {10.1016/j.ejor.2018.05.041},
	abstract = {The Rapid Transit Network Design planning problem along a time horizon is treated by considering uncertainty in passenger demand, strategic costs and network disruption. The problem has strategic decisions about the timing to construct stations and edges, and operational decisions on the available network at the periods. The uncertainty in the strategic side is represented in a multistage scenario tree, while the uncertainty in the operational side is represented in two-stage scenario trees which are rooted with strategic nodes. The 0–1 deterministic equivalent model can have very large dimensions. So-called fix-and-relax and lazy matheuristic algorithms, which are based on special features of the problem, are proposed, jointly with dynamic scenario aggregation/de-aggregation schemes. A broad computational experience is presented by considering a network case study taken from the literature, where the problem was only treated as a deterministic 0–1 model. 40 nodes in the strategic multistage tree are considered for passenger demand and investment cost and 8 uncertainties are considered for network disruption in each strategic node, in total 320 uncertain situations are jointly considered. For assessing the validity of the proposal, a computational comparison is performed between the plain use of a state-of-the-art optimization solver and the proposals made in this work. The model is so-large (2.6M constraints and 1.6M binary variables) that the solver alone cannot provide a solution in an affordable time. However, a mixture of the both matheuristics provides a solution with a good optimality gap requiring an affordable elapsed time.},
	number = {2},
	urldate = {2023-09-12},
	journal = {European Journal of Operational Research},
	author = {Cadarso, Luis and Escudero, Laureano F. and Marín, Angel},
	month = dec,
	year = {2018},
	keywords = {Matheuristic algorithms, Pure 0–1 models, Rapid Transit Network Design planning, Transportation, multistage multi-horizon scenario trees},
	pages = {577--593},
}

@inproceedings{gendron_relaxations_1994,
	title = {{RELAXATIONS} {FOR} {MULTICOMMODITY} {CAPACITATED} {NETWORK} {DESIGN} {PROBLEMS}.},
	url = {https://www.semanticscholar.org/paper/RELAXATIONS-FOR-MULTICOMMODITY-CAPACITATED-NETWORK-Gendron-Crainic/ec9df1ad7ccdca7033afc6f7a2526bdc14849a36},
	abstract = {Semantic Scholar extracted view of "RELAXATIONS FOR MULTICOMMODITY CAPACITATED NETWORK DESIGN PROBLEMS." by B. Gendron et al.},
	urldate = {2023-09-12},
	author = {Gendron, B. and Crainic, T.},
	year = {1994},
	keywords = {nd},
}

@article{gomory_multi-terminal_1961,
	title = {Multi-{Terminal} {Network} {Flows}},
	volume = {9},
	issn = {0368-4245, 2168-3484},
	url = {http://epubs.siam.org/doi/10.1137/0109047},
	doi = {10.1137/0109047},
	language = {en},
	number = {4},
	urldate = {2023-09-11},
	journal = {Journal of the Society for Industrial and Applied Mathematics},
	author = {Gomory, R. E. and Hu, T. C.},
	month = dec,
	year = {1961},
	pages = {551--570},
}

@article{hirsch_fixed_1968,
	title = {The fixed charge problem},
	volume = {15},
	copyright = {Copyright © 1968 Wiley Periodicals, Inc., A Wiley Company},
	issn = {1931-9193},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/nav.3800150306},
	doi = {10.1002/nav.3800150306},
	abstract = {A fundamental unsolved problem in the programming area is one in which various activities have fixed charges (e.g., set-up time charges) if operating at a positive level. Properties of a general solution to this type problem are discussed in this paper. Under special circumstances it is shown that a fixed charge problem can be reduced to an ordinary linear programming problem.},
	language = {en},
	number = {3},
	urldate = {2023-09-11},
	journal = {Naval Research Logistics Quarterly},
	author = {Hirsch, Warren M. and Dantzig, George B.},
	year = {1968},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/nav.3800150306},
	keywords = {nd},
	pages = {413--424},
}

@article{koster_robust_2013,
	title = {Robust {Network} {Design}: {Formulations}, {Valid} {Inequalities}, and {Computations}},
	volume = {61},
	shorttitle = {Robust {Network} {Design}},
	doi = {10.1002/net.21497},
	abstract = {Traffic in communication networks fluctuates heavily over time. Thus, to avoid capacity bottlenecks, operators highly overestimate the traffic volume during network planning. In this article we consider telecommunication network design under traffic uncertainty, adapting the robust optimization approach of Bertsimas and Sim [Oper Res 52 (2004), 35–53]. We present two different mathematical formulations for this problem, provide valid inequalities, study the computational implications, and evaluate the realized robustness. To enhance the performance of the mixed-integer programming solver, we derive robust cutset inequalities generalizing their deterministic counterparts. Instead of a single cutset inequality for every network cut, we derive multiple valid inequalities by exploiting the extra variables available in the robust formulations. We show that these inequalities define facets under certain conditions and that they completely describe a projection of the robust cutset polyhedron if the cutset consists of a single edge. For realistic networks and live traffic measurements, we compare the formulations and report on the speed-up achieved by the valid inequalities. We study the “price of robustness” and evaluate the approach by analyzing the real network load. The results show that the robust optimization approach has the potential to support network planners better than present methods. © 2013 Wiley Periodicals, Inc. NETWORKS, 2013},
	journal = {Networks},
	author = {Koster, Arie and Kutschka, Manuel and Raack, Christian},
	month = mar,
	year = {2013},
}

@article{alonso-ayuso_approach_2003,
	title = {An {Approach} for {Strategic} {Supply} {Chain} {Planning} under {Uncertainty} based on {Stochastic} 0-1 {Programming}},
	volume = {26},
	issn = {1573-2916},
	url = {https://doi.org/10.1023/A:1023071216923},
	doi = {10.1023/A:1023071216923},
	abstract = {We present a two-stage stochastic 0-1 modeling and a related algorithmic approach for Supply Chain Management under uncertainty, whose goal consists of determining the production topology, plant sizing, product selection, product allocation among plants and vendor selection for raw materials. The objective is the maximization of the expected benefit given by the product net profit over the time horizon minus the investment depreciation and operations costs. The main uncertain parameters are the product net price and demand, the raw material supply cost and the production cost. The first stage is included by the strategic decisions. The second stage is included by the tactical decisions. A tight 0-1 model for the deterministic version is presented. A splitting variable mathematical representation via scenario is presented for the stochastic version of the model. A two-stage version of a Branch and Fix Coordination (BFC) algorithmic approach is proposed for stochastic 0-1 program solving, and some computational experience is reported for cases with dozens of thousands of constraints and continuous variables and hundreds of 0-1 variables.},
	language = {en},
	number = {1},
	urldate = {2023-09-11},
	journal = {Journal of Global Optimization},
	author = {Alonso-Ayuso, A. and Escudero, L.F. and Garín, A. and Ortuño, M.T. and Pérez, G.},
	month = may,
	year = {2003},
	keywords = {BoM, Branch-and-Fix Coordination, Plant sizing, Splitting variable, Strategic planning, Supply chain, Two-stage stochastic, Vendor selection},
	pages = {97--124},
}

@article{de_gooijer_25_2006,
	series = {Twenty five years of forecasting},
	title = {25 years of time series forecasting},
	volume = {22},
	issn = {0169-2070},
	url = {https://www.sciencedirect.com/science/article/pii/S0169207006000021},
	doi = {10.1016/j.ijforecast.2006.01.001},
	abstract = {We review the past 25 years of research into time series forecasting. In this silver jubilee issue, we naturally highlight results published in journals managed by the International Institute of Forecasters (Journal of Forecasting 1982–1985 and International Journal of Forecasting 1985–2005). During this period, over one third of all papers published in these journals concerned time series forecasting. We also review highly influential works on time series forecasting that have been published elsewhere during this period. Enormous progress has been made in many areas, but we find that there are a large number of topics in need of further development. We conclude with comments on possible future research directions in this field.},
	language = {en},
	number = {3},
	urldate = {2023-08-09},
	journal = {International Journal of Forecasting},
	author = {De Gooijer, Jan G. and Hyndman, Rob J.},
	month = jan,
	year = {2006},
	keywords = {ARCH, ARIMA, Accuracy measures, Combining, Count data, Densities, Exponential smoothing, Kalman filter, Long memory, Multivariate, Neural nets, Nonlinearity, Prediction intervals, Regime-switching, Robustness, Seasonality, State space, Structural models, Transfer function, Univariate, VAR},
	pages = {443--473},
}

@book{bishop_pattern_2006,
	address = {Berlin, Heidelberg},
	title = {Pattern {Recognition} and {Machine} {Learning} ({Information} {Science} and {Statistics})},
	isbn = {978-0-387-31073-2},
	publisher = {Springer-Verlag},
	author = {Bishop, Christopher M.},
	month = jul,
	year = {2006},
}

@misc{besbes_contextual_2023,
	title = {Contextual {Inverse} {Optimization}: {Offline} and {Online} {Learning}},
	shorttitle = {Contextual {Inverse} {Optimization}},
	url = {http://arxiv.org/abs/2106.14015},
	doi = {10.48550/arXiv.2106.14015},
	abstract = {We study the problems of offline and online contextual optimization with feedback information, where instead of observing the loss, we observe, after-the-fact, the optimal action an oracle with full knowledge of the objective function would have taken. We aim to minimize regret, which is defined as the difference between our losses and the ones incurred by an all-knowing oracle. In the offline setting, the decision-maker has information available from past periods and needs to make one decision, while in the online setting, the decision-maker optimizes decisions dynamically over time based a new set of feasible actions and contextual functions in each period. For the offline setting, we characterize the optimal minimax policy, establishing the performance that can be achieved as a function of the underlying geometry of the information induced by the data. In the online setting, we leverage this geometric characterization to optimize the cumulative regret. We develop an algorithm that yields the first regret bound for this problem that is logarithmic in the time horizon. Finally, we show via simulation that our proposed algorithms outperform previous methods from the literature.},
	urldate = {2023-07-12},
	publisher = {arXiv},
	author = {Besbes, Omar and Fonseca, Yuri and Lobel, Ilan},
	month = jul,
	year = {2023},
	note = {arXiv:2106.14015 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@misc{sun_maximum_2023,
	title = {Maximum {Optimality} {Margin}: {A} {Unified} {Approach} for {Contextual} {Linear} {Programming} and {Inverse} {Linear} {Programming}},
	shorttitle = {Maximum {Optimality} {Margin}},
	url = {http://arxiv.org/abs/2301.11260},
	doi = {10.48550/arXiv.2301.11260},
	abstract = {In this paper, we study the predict-then-optimize problem where the output of a machine learning prediction task is used as the input of some downstream optimization problem, say, the objective coefficient vector of a linear program. The problem is also known as predictive analytics or contextual linear programming. The existing approaches largely suffer from either (i) optimization intractability (a non-convex objective function)/statistical inefficiency (a suboptimal generalization bound) or (ii) requiring strong condition(s) such as no constraint or loss calibration. We develop a new approach to the problem called {\textbackslash}textit\{maximum optimality margin\} which designs the machine learning loss function by the optimality condition of the downstream optimization. The max-margin formulation enjoys both computational efficiency and good theoretical properties for the learning procedure. More importantly, our new approach only needs the observations of the optimal solution in the training data rather than the objective function, which makes it a new and natural approach to the inverse linear programming problem under both contextual and context-free settings; we also analyze the proposed method under both offline and online settings, and demonstrate its performance using numerical experiments.},
	urldate = {2023-07-12},
	publisher = {arXiv},
	author = {Sun, Chunlin and Liu, Shang and Li, Xiaocheng},
	month = may,
	year = {2023},
	note = {arXiv:2301.11260 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@misc{mulamba_contrastive_2021,
	title = {Contrastive {Losses} and {Solution} {Caching} for {Predict}-and-{Optimize}},
	url = {http://arxiv.org/abs/2011.05354},
	doi = {10.48550/arXiv.2011.05354},
	abstract = {Many decision-making processes involve solving a combinatorial optimization problem with uncertain input that can be estimated from historic data. Recently, problems in this class have been successfully addressed via end-to-end learning approaches, which rely on solving one optimization problem for each training instance at every epoch. In this context, we provide two distinct contributions. First, we use a Noise Contrastive approach to motivate a family of surrogate loss functions, based on viewing non-optimal solutions as negative examples. Second, we address a major bottleneck of all predict-and-optimize approaches, i.e. the need to frequently recompute optimal solutions at training time. This is done via a solver-agnostic solution caching scheme, and by replacing optimization calls with a lookup in the solution cache. The method is formally based on an inner approximation of the feasible space and, combined with a cache lookup strategy, provides a controllable trade-off between training time and accuracy of the loss approximation. We empirically show that even a very slow growth rate is enough to match the quality of state-of-the-art methods, at a fraction of the computational cost.},
	urldate = {2023-07-07},
	publisher = {arXiv},
	author = {Mulamba, Maxime and Mandi, Jayanta and Diligenti, Michelangelo and Lombardi, Michele and Bucarey, Victor and Guns, Tias},
	month = jul,
	year = {2021},
	note = {arXiv:2011.05354 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{amos_optnet_2021,
	title = {{OptNet}: {Differentiable} {Optimization} as a {Layer} in {Neural} {Networks}},
	shorttitle = {{OptNet}},
	url = {http://arxiv.org/abs/1703.00443},
	abstract = {This paper presents OptNet, a network architecture that integrates optimization problems (here, speciﬁcally in the form of quadratic programs) as individual layers in larger end-to-end trainable deep networks. These layers encode constraints and complex dependencies between the hidden states that traditional convolutional and fully-connected layers often cannot capture. We explore the foundations for such an architecture: we show how techniques from sensitivity analysis, bilevel optimization, and implicit differentiation can be used to exactly differentiate through these layers and with respect to layer parameters; we develop a highly efﬁcient solver for these layers that exploits fast GPU-based batch solves within a primal-dual interior point method, and which provides backpropagation gradients with virtually no additional cost on top of the solve; and we highlight the application of these approaches in several problems. In one notable example, the method is learns to play mini-Sudoku (4x4) given just input and output games, with no a-priori information about the rules of the game; this highlights the ability of OptNet to learn hard constraints better than other neural architectures.},
	language = {en},
	urldate = {2023-07-06},
	publisher = {arXiv},
	author = {Amos, Brandon and Kolter, J. Zico},
	month = dec,
	year = {2021},
	note = {arXiv:1703.00443 [cs, math, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@misc{laage_periodic_2022,
	title = {Periodic {Freight} {Demand} {Estimation} for {Large}-scale {Tactical} {Planning}},
	url = {http://arxiv.org/abs/2105.09136},
	doi = {10.48550/arXiv.2105.09136},
	abstract = {Freight carriers rely on tactical planning to design their service network to satisfy demand in a cost-effective way. For computational tractability, deterministic and cyclic Service Network Design (SND) formulations are used to solve large-scale problems. A central input is the periodic demand, that is, the demand expected to repeat in every period in the planning horizon. In practice, demand is predicted by a time series forecasting model and the periodic demand is the average of those forecasts. This is, however, only one of many possible mappings. The problem consisting in selecting this mapping has hitherto been overlooked in the literature. We propose to use the structure of the downstream decision-making problem to select a good mapping. For this purpose, we introduce a multilevel mathematical programming formulation that explicitly links the time series forecasts to the SND problem of interest. The solution is a periodic demand estimate that minimizes costs over the tactical planning horizon. We report results in an extensive empirical study of a large-scale application from the Canadian National Railway Company. They clearly show the importance of the periodic demand estimation problem. Indeed, the planning costs exhibit an important variation over different periodic demand estimates and using an estimate different from the mean forecast can lead to substantial cost reductions. Moreover, the costs associated with the periodic demand estimates based on forecasts were comparable to, or even better than those obtained using the mean of actual demand.},
	urldate = {2023-07-06},
	publisher = {arXiv},
	author = {Laage, Greta and Frejinger, Emma and Savard, Gilles},
	month = jan,
	year = {2022},
	note = {arXiv:2105.09136 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control},
}

@misc{sadana_survey_2023,
	title = {A {Survey} of {Contextual} {Optimization} {Methods} for {Decision} {Making} under {Uncertainty}},
	url = {http://arxiv.org/abs/2306.10374},
	doi = {10.48550/arXiv.2306.10374},
	abstract = {Recently there has been a surge of interest in operations research (OR) and the machine learning (ML) community in combining prediction algorithms and optimization techniques to solve decision-making problems in the face of uncertainty. This gave rise to the field of contextual optimization, under which data-driven procedures are developed to prescribe actions to the decision-maker that make the best use of the most recently updated information. A large variety of models and methods have been presented in both OR and ML literature under a variety of names, including data-driven optimization, prescriptive optimization, predictive stochastic programming, policy optimization, (smart) predict/estimate-then-optimize, decision-focused learning, (task-based) end-to-end learning/forecasting/optimization, etc. Focusing on single and two-stage stochastic programming problems, this review article identifies three main frameworks for learning policies from data and discusses their strengths and limitations. We present the existing models and methods under a uniform notation and terminology and classify them according to the three main frameworks identified. Our objective with this survey is to both strengthen the general understanding of this active field of research and stimulate further theoretical and algorithmic advancements in integrating ML and stochastic programming.},
	urldate = {2023-07-05},
	publisher = {arXiv},
	author = {Sadana, Utsav and Chenreddy, Abhilash and Delage, Erick and Forel, Alexandre and Frejinger, Emma and Vidal, Thibaut},
	month = jun,
	year = {2023},
	note = {arXiv:2306.10374 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control},
}

@misc{noauthor_230610374_nodate,
	title = {[2306.10374] {A} {Survey} of {Contextual} {Optimization} {Methods} for {Decision} {Making} under {Uncertainty}},
	url = {https://arxiv.org/abs/2306.10374},
	urldate = {2023-07-05},
}

@article{burton_instance_1992,
	title = {On an instance of the inverse shortest paths problem},
	volume = {53},
	issn = {1436-4646},
	url = {https://doi.org/10.1007/BF01585693},
	doi = {10.1007/BF01585693},
	abstract = {The inverse shortest paths problem in a graph is considered, that is, the problem of recovering the arc costs given some information about the shortest paths in the graph. The problem is first motivated by some practical examples arising from applications. An algorithm based on the Goldfarb-Idnani method for convex quadratic programming is then proposed and analyzed for one of the instances of the problem. Preliminary numerical results are reported.},
	language = {en},
	number = {1},
	urldate = {2023-06-13},
	journal = {Mathematical Programming},
	author = {Burton, D. and Toint, Ph. L.},
	month = jan,
	year = {1992},
	keywords = {Graph theory, inverse problems, invopt; unread, quadratic programming, shortest paths},
	pages = {45--61},
}

@article{hewitt_data-driven_2020,
	title = {Data-driven optimization model customization},
	volume = {287},
	issn = {0377-2217},
	url = {https://www.sciencedirect.com/science/article/pii/S0377221720304215},
	doi = {10.1016/j.ejor.2020.05.010},
	abstract = {When embedded in software-based decision support systems, optimization models can greatly improve organizational planning. In many industries, there are classical models that capture the fundamentals of general planning decisions (e.g., designing a delivery route). However, these models are generic and often require customization to truly reflect the realities of specific operational settings. Yet, such customization can be an expensive and time-consuming process. At the same time, popular cloud computing software platforms such as Software as a Service (SaaS) are not amenable to customized software applications. We present a framework that has the potential to autonomously customize optimization models by learning mathematical representations of customer-specific business rules from historical data derived from model solutions and implemented plans. Because of the wide-spread use in practice of mixed integer linear programs (MILP) and the power of MILP solvers, the framework is designed for MILP models. It uses a common mathematical representation for different optimization models and business rules, which it encodes in a standard data structure. As a result, a software provider employing this framework can develop and maintain a single code-base while meeting the needs of different customers. We assess the effectiveness of this framework on multiple classical MILPs used in the planning of logistics and supply chain operations and with different business rules that must be observed by implementable plans. Computational experiments based on synthetic data indicate that solutions to the customized optimization models produced by the framework are regularly of high-quality.},
	language = {en},
	number = {2},
	urldate = {2023-05-26},
	journal = {European Journal of Operational Research},
	author = {Hewitt, Mike and Frejinger, Emma},
	month = dec,
	year = {2020},
	keywords = {Decision support systems, Mixed integer linear programming, Optimization modeling, Statistical learning},
	pages = {438--451},
}

@article{fajemisin_optimization_2023,
	title = {Optimization with constraint learning: {A} framework and survey},
	issn = {0377-2217},
	shorttitle = {Optimization with constraint learning},
	url = {https://www.sciencedirect.com/science/article/pii/S0377221723003405},
	doi = {10.1016/j.ejor.2023.04.041},
	abstract = {Many real-life optimization problems frequently contain one or more constraints or objectives for which there are no explicit formulae. If however data on feasible and/or infeasible states are available, these data can be used to learn the constraints. The benefits of this approach are clearly seen, however, there is a need for this process to be carried out in a structured manner. This paper, therefore, provides a framework for Optimization with Constraint Learning (OCL) which we believe will help to formalize and direct the process of learning constraints from data. This framework includes the following steps: (i) setup of the conceptual optimization model, (ii) data gathering and preprocessing, (iii) selection and training of predictive models, (iv) resolution of the optimization model, and (v) verification and improvement of the optimization model. We then review the recent OCL literature in light of this framework and highlight current trends, as well as areas for future research.},
	language = {en},
	urldate = {2023-05-19},
	journal = {European Journal of Operational Research},
	author = {Fajemisin, Adejuyigbe O. and Maragno, Donato and den Hertog, Dick},
	month = may,
	year = {2023},
	keywords = {Analytics, Constraint learning, Machine learning, Optimization, promising, unread},
}

@misc{tan_deep_2018,
	title = {Deep {Inverse} {Optimization}},
	url = {http://arxiv.org/abs/1812.00804},
	doi = {10.48550/arXiv.1812.00804},
	abstract = {Given a set of observations generated by an optimization process, the goal of inverse optimization is to determine likely parameters of that process. We cast inverse optimization as a form of deep learning. Our method, called deep inverse optimization, is to unroll an iterative optimization process and then use backpropagation to learn parameters that generate the observations. We demonstrate that by backpropagating through the interior point algorithm we can learn the coefficients determining the cost vector and the constraints, independently or jointly, for both non-parametric and parametric linear programs, starting from one or multiple observations. With this approach, inverse optimization can leverage concepts and algorithms from deep learning.},
	urldate = {2023-05-03},
	publisher = {arXiv},
	author = {Tan, Yingcong and Delong, Andrew and Terekhov, Daria},
	month = dec,
	year = {2018},
	note = {arXiv:1812.00804 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning, invopt, unread},
}

@article{ahuja_combinatorial_2002,
	title = {Combinatorial algorithms for inverse network flow problems},
	volume = {40},
	issn = {1097-0037},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/net.10048},
	doi = {10.1002/net.10048},
	abstract = {An inverse optimization problem is defined as follows: Let S denote the set of feasible solutions of an optimization problem P, let c be a specified cost vector, and x0 ∈ S. We want to perturb the cost vector c to d so that x0 is an optimal solution of P with respect to the cost vector d, and w∥d − c∥p is minimum, where ∥ · ∥p denotes some selected lp norm and w is a vector of weights. In this paper, we consider inverse minimum-cut and minimum-cost flow problems under the l1 normal (where the objective is to minimize ∑j∈Jwj{\textbar}dj − cj{\textbar} for some index set J of variables) and under the l∞ norm (where the objective is to minimize maxwj{\textbar}dj − cj{\textbar}: j ∈ J). We show that the unit weight (i.e., wj = 1 for all j ∈ J) inverse minimum-cut problem under the l1 norm reduces to solving a maximum-flow problem, and under the l∞ norm, it requires solving a polynomial sequence of minimum-cut problems. The unit weight inverse minimum-cost flow problem under the l1 norm reduces to solving a unit capacity minimum-cost circulation problem, and under the l∞ norm, it reduces to solving a minimum mean cycle problem. We also consider the nonunit weight versions of inverse minimum-cut and minimum-cost flow problems under the l∞ norm. © 2002 Wiley Periodicals, Inc.},
	language = {en},
	number = {4},
	urldate = {2023-05-02},
	journal = {Networks},
	author = {Ahuja, Ravindra K. and Orlin, James B.},
	year = {2002},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/net.10048},
	keywords = {inverse optimization, invopt, maximum-flow problem, minimax problems, minimum mean-cycle problem, minimum-cost flow problem, minimum-cut problem, promising, unread},
	pages = {181--187},
}

@article{guler_capacity_2010,
	title = {Capacity inverse minimum cost flow problem},
	volume = {19},
	issn = {1573-2886},
	url = {https://doi.org/10.1007/s10878-008-9159-8},
	doi = {10.1007/s10878-008-9159-8},
	abstract = {Given a directed graph G=(N,A) with arc capacities uijand a minimum cost flow problem defined on G, the capacity inverse minimum cost flow problem is to find a new capacity vector \${\textbackslash}hat\{u\}\$for the arc set A such that a given feasible flow \${\textbackslash}hat\{x\}\$is optimal with respect to the modified capacities. Among all capacity vectors \${\textbackslash}hat\{u\}\$satisfying this condition, we would like to find one with minimum \${\textbackslash}{\textbar}{\textbackslash}hat\{u\}-u{\textbackslash}{\textbar}\$value.},
	language = {en},
	number = {1},
	urldate = {2023-05-02},
	journal = {Journal of Combinatorial Optimization},
	author = {Güler, Çiğdem and Hamacher, Horst W.},
	month = jan,
	year = {2010},
	keywords = {Inverse problems, Minimum cost flows, Network flows, invopt, promising, unread},
	pages = {43--59},
}

@article{ghobadi_inferring_2021,
	title = {Inferring linear feasible regions using inverse optimization},
	volume = {290},
	issn = {0377-2217},
	url = {https://www.sciencedirect.com/science/article/pii/S037722172030761X},
	doi = {10.1016/j.ejor.2020.08.048},
	abstract = {Consider a problem where a set of feasible observations are provided by an expert, and a cost function exists that characterizes which of the observations dominate the others and are hence, preferred. Assume the expert has an implicit optimization model in mind to identify the feasible observations, but the explicit constraints of this underlying model are unknown. Our goal is to infer the feasible region of such an optimization model that would render these observations feasible while making the best ones optimal for the cost (objective) function. Such feasible regions (i) build a baseline for a systematic categorization of future observations, and (ii) allow for using sensitivity analysis to discern changes in optimal solutions if the objective function changes in the future. In this paper, we propose a general inverse optimization methodology that recovers the complete constraint matrix of a linear model and then introduce a tractable equivalent reformulation. Furthermore, we provide and discuss several generalized loss functions to inform the desirable properties of the feasible region based on user preference and historical data. We demonstrate our approach using numerical examples and a realistic diet recommendation case study for imputing personalized diets. Our numerical examples verify the validity of our approach and emphasize the differences among the proposed loss functions. The diet recommendation case study shows that the proposed models can improve the palatability of the recommended diets for each user, and it provides further intuition for large-scale implementations of the proposed methodology.},
	language = {en},
	number = {3},
	urldate = {2023-04-04},
	journal = {European Journal of Operational Research},
	author = {Ghobadi, Kimia and Mahmoudzadeh, Houra},
	month = may,
	year = {2021},
	keywords = {Diet recommendation, Feasible region inference, Inverse optimization, Linear programming, Loss function, invopt, read},
	pages = {829--843},
}

@article{bulut_complexity_2021,
	title = {On the {Complexity} of {Inverse} {Mixed} {Integer} {Linear} {Optimization}},
	volume = {31},
	issn = {1052-6234, 1095-7189},
	url = {https://epubs.siam.org/doi/10.1137/20M1377369},
	doi = {10.1137/20M1377369},
	abstract = {Inverse optimization is the problem of determining the values of missing input parameters for an associated forward problem that are closest to given estimates and that will make a given target vector optimal. This study is concerned with the relationship of a particular inverse mixed integer linear optimization problem (MILP) to both the forward problem and the separation problem associated with its feasible region. We show that a decision version of the inverse MILP in which a primal bound is verified is coNP-complete, whereas primal bound verification for the associated forward problem is NP-complete, and that the optimal value verification problems for both the inverse problem and the associated forward problem are complete for the complexity class DP. We also describe a cutting-plane algorithm for solving inverse MILPs that illustrates the close relationship between the separation problem for the convex hull of solutions to a given MILP and the associated inverse problem. The inverse problem is shown to be equivalent to the separation problem for the radial cone defined by all inequalities that are both valid for the convex hull of solutions to the forward problem and binding at the target vector. Thus, the inverse, forward, and separation problems can be said to be equivalent.},
	language = {en},
	number = {4},
	urldate = {2023-05-01},
	journal = {SIAM Journal on Optimization},
	author = {Bulut, Aykut and Ralphs, Ted K.},
	month = jan,
	year = {2021},
	keywords = {promising, unread},
	pages = {3014--3043},
}

@misc{mandi_decision-focused_2022,
	title = {Decision-{Focused} {Learning}: {Through} the {Lens} of {Learning} to {Rank}},
	shorttitle = {Decision-{Focused} {Learning}},
	url = {http://arxiv.org/abs/2112.03609},
	doi = {10.48550/arXiv.2112.03609},
	abstract = {In the last years decision-focused learning framework, also known as predict-and-optimize, have received increasing attention. In this setting, the predictions of a machine learning model are used as estimated cost coefficients in the objective function of a discrete combinatorial optimization problem for decision making. Decision-focused learning proposes to train the ML models, often neural network models, by directly optimizing the quality of decisions made by the optimization solvers. Based on a recent work that proposed a noise contrastive estimation loss over a subset of the solution space, we observe that decision-focused learning can more generally be seen as a learning-to-rank problem, where the goal is to learn an objective function that ranks the feasible points correctly. This observation is independent of the optimization method used and of the form of the objective function. We develop pointwise, pairwise and listwise ranking loss functions, which can be differentiated in closed form given a subset of solutions. We empirically investigate the quality of our generic methods compared to existing decision-focused learning approaches with competitive results. Furthermore, controlling the subset of solutions allows controlling the runtime considerably, with limited effect on regret.},
	urldate = {2023-05-01},
	publisher = {arXiv},
	author = {Mandi, Jayanta and Bucarey, Víctor and Mulamba, Maxime and Guns, Tias},
	month = jun,
	year = {2022},
	note = {arXiv:2112.03609 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, promising, unread},
}

@misc{laage_two-step_2021,
	title = {A {Two}-step {Heuristic} for the {Periodic} {Demand} {Estimation} {Problem}},
	url = {http://arxiv.org/abs/2108.08331},
	doi = {10.48550/arXiv.2108.08331},
	abstract = {Freight carriers rely on tactical plans to satisfy demand in a cost-effective way. For computational tractability in real large-scale settings, such plans are typically computed by solving deterministic and cyclic formulations. An important input is the periodic demand, i.e., the demand that is expected to repeat in each period of the planning horizon. Motivated by the discrepancy between time series forecasts of demand in each period and the periodic demand, Laage et al. (2021) recently introduced the Periodic Demand Estimation (PDE) problem and showed that it has a high value. However, they made strong assumptions on the solution space so that the problem could be solved by enumeration. In this paper we significantly extend their work. We propose a new PDE formulation that relaxes the strong assumptions on the solution space. We solve large instances of this formulation with a two-step heuristic. The first step reduces the dimension of the feasible space by performing clustering of commodities based on instance-specific information about demand and supply interactions. The formulation along with the first step allow to solve the problem in a second step by either metaheuristics or the state-of-the-art black-box optimization solver NOMAD. In an extensive empirical study using real data from the Canadian National Railway Company, we show that our methodology produces high quality solutions and outperforms existing ones.},
	urldate = {2023-04-03},
	publisher = {arXiv},
	author = {Laage, Greta and Frejinger, Emma and Savard, Gilles},
	month = aug,
	year = {2021},
	note = {Number: arXiv:2108.08331
arXiv:2108.08331 [cs, math]},
	keywords = {Computer Science - Other Computer Science, Mathematics - Optimization and Control, prethesis, read},
}

@article{pougala_capturing_2021,
	title = {Capturing trade-offs between daily scheduling choices},
	language = {en},
	author = {Pougala, Janody and Hillel, Tim and Bierlaire, Michel},
	month = jan,
	year = {2021},
	pages = {27},
}

@article{pape_constraint-based_nodate,
	title = {Constraint-{Based} {Scheduling}},
	language = {en},
	author = {Pape, Claude Le},
	pages = {33},
}

@article{elmachtoub_smart_2022,
	title = {Smart “{Predict}, then {Optimize}”},
	volume = {68},
	issn = {0025-1909},
	url = {https://pubsonline.informs.org/doi/abs/10.1287/mnsc.2020.3922},
	doi = {10.1287/mnsc.2020.3922},
	abstract = {Many real-world analytics problems involve two significant challenges: prediction and optimization. Because of the typically complex nature of each challenge, the standard paradigm is predict-then-optimize. By and large, machine learning tools are intended to minimize prediction error and do not account for how the predictions will be used in the downstream optimization problem. In contrast, we propose a new and very general framework, called Smart “Predict, then Optimize” (SPO), which directly leverages the optimization problem structure—that is, its objective and constraints—for designing better prediction models. A key component of our framework is the SPO loss function, which measures the decision error induced by a prediction. Training a prediction model with respect to the SPO loss is computationally challenging, and, thus, we derive, using duality theory, a convex surrogate loss function, which we call the SPO+ loss. Most importantly, we prove that the SPO+ loss is statistically consistent with respect to the SPO loss under mild conditions. Our SPO+ loss function can tractably handle any polyhedral, convex, or even mixed-integer optimization problem with a linear objective. Numerical experiments on shortest-path and portfolio-optimization problems show that the SPO framework can lead to significant improvement under the predict-then-optimize paradigm, in particular, when the prediction model being trained is misspecified. We find that linear models trained using SPO+ loss tend to dominate random-forest algorithms, even when the ground truth is highly nonlinear.

This paper was accepted by Yinyu Ye, optimization.

Supplemental Material: Data and the online appendix are available at https://doi.org/10.1287/mnsc.2020.3922},
	number = {1},
	urldate = {2023-04-03},
	journal = {Management Science},
	author = {Elmachtoub, Adam N. and Grigas, Paul},
	month = jan,
	year = {2022},
	note = {Publisher: INFORMS},
	keywords = {data-driven optimization, linear regression, machine learning, prescriptive analytics, prethesis, read, spo},
	pages = {9--26},
}

@article{naderi_mixed-integer_nodate,
	title = {Mixed-{Integer} {Programming} {Versus} {Constraint} {Programming} for {Shop} {Scheduling} {Problems}: {New} {Results} and {Outlook}},
	abstract = {Constraint Programming (CP) has been given a new lease of life after new CP-based procedures have been incorporated into state-of-the-art solvers, most notably the CP Optimizer from IBM. Classical CP solvers were only capable of guaranteeing the optimality of a solution, but they could not provide bounds for the integer feasible solutions found if interrupted prematurely due to, say, timelimits. New versions, however, provide bounds and optimality guarantees, eﬀectively making CP a viable alternative to more traditional mixed-integer programming (MIP) models and solvers. We capitalize on these developments and conduct a computational evaluation of MIP and CP models on 12 select scheduling problems.1 We carefully chose these 12 problems to represent a wide variety of scheduling problems that occur in diﬀerent service and manufacturing settings. We also consider basic and well-studied simpliﬁed problems. These scheduling settings range from pure sequencing (e.g., ﬂow shop and open shop) or joint assignment-sequencing (e.g., distributed ﬂow shop and hybrid ﬂow shop) to pure assignment (i.e., parallel machine) scheduling problems. We present MIP and CP models for each variant of these problems and evaluate their performance over 17 relevant and standard benchmarks that we identiﬁed in the literature. The computational campaign encompasses almost 6,623 experiments and evaluates the MIP and CP models along ﬁve dimensions of problem characteristics, objective function, decision variables, input parameters, and quality of bounds. We establish the areas that each one of these models performs well and recognize their conceivable reasons. The obtained results indicate that CP sets new limits concerning the maximum problem size that can be solved using oﬀ-the-shelf exact techniques.},
	language = {en},
	author = {Naderi, Bahman and Ruiz, Ruben and Roshanaei, Vahid},
	pages = {43},
}

@misc{wilder_melding_2018,
	title = {Melding the {Data}-{Decisions} {Pipeline}: {Decision}-{Focused} {Learning} for {Combinatorial} {Optimization}},
	shorttitle = {Melding the {Data}-{Decisions} {Pipeline}},
	url = {http://arxiv.org/abs/1809.05504},
	doi = {10.48550/arXiv.1809.05504},
	abstract = {Creating impact in real-world settings requires artificial intelligence techniques to span the full pipeline from data, to predictive models, to decisions. These components are typically approached separately: a machine learning model is first trained via a measure of predictive accuracy, and then its predictions are used as input into an optimization algorithm which produces a decision. However, the loss function used to train the model may easily be misaligned with the end goal, which is to make the best decisions possible. Hand-tuning the loss function to align with optimization is a difficult and error-prone process (which is often skipped entirely). We focus on combinatorial optimization problems and introduce a general framework for decision-focused learning, where the machine learning model is directly trained in conjunction with the optimization algorithm to produce high-quality decisions. Technically, our contribution is a means of integrating common classes of discrete optimization problems into deep learning or other predictive models, which are typically trained via gradient descent. The main idea is to use a continuous relaxation of the discrete problem to propagate gradients through the optimization procedure. We instantiate this framework for two broad classes of combinatorial problems: linear programs and submodular maximization. Experimental results across a variety of domains show that decision-focused learning often leads to improved optimization performance compared to traditional methods. We find that standard measures of accuracy are not a reliable proxy for a predictive model's utility in optimization, and our method's ability to specify the true goal as the model's training objective yields substantial dividends across a range of decision problems.},
	urldate = {2023-04-03},
	publisher = {arXiv},
	author = {Wilder, Bryan and Dilkina, Bistra and Tambe, Milind},
	month = nov,
	year = {2018},
	note = {Number: arXiv:1809.05504
arXiv:1809.05504 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, read, researchproposal, unread},
}

@misc{chan_inverse_2022,
	title = {Inverse {Optimization}: {Theory} and {Applications}},
	shorttitle = {Inverse {Optimization}},
	url = {http://arxiv.org/abs/2109.03920},
	doi = {10.48550/arXiv.2109.03920},
	abstract = {Inverse optimization describes a process that is the "reverse" of traditional mathematical optimization. Unlike traditional optimization, which seeks to compute optimal decisions given an objective and constraints, inverse optimization takes decisions as input and determines an objective and/or constraints that render these decisions approximately or exactly optimal. In recent years, there has been an explosion of interest in the mathematics and applications of inverse optimization. This paper provides a comprehensive review of both the methodological and application-oriented literature in inverse optimization.},
	urldate = {2023-04-04},
	publisher = {arXiv},
	author = {Chan, Timothy C. Y. and Mahmood, Rafid and Zhu, Ian Yihang},
	month = jul,
	year = {2022},
	note = {arXiv:2109.03920 [math]},
	keywords = {Mathematics - Optimization and Control, invopt, lessrel, read, researchproposal-lessrel},
}

@article{chan_inverse_2020,
	title = {Inverse optimization for the recovery of constraint parameters},
	volume = {282},
	issn = {0377-2217},
	url = {https://www.sciencedirect.com/science/article/pii/S0377221719307830},
	doi = {10.1016/j.ejor.2019.09.027},
	abstract = {Most inverse optimization models impute unspecified parameters of an objective function to make an observed solution optimal for a given optimization problem with a fixed feasible set. We propose two approaches to impute unspecified left-hand-side constraint coefficients in addition to a cost vector for a given linear optimization problem. The first approach identifies parameters minimizing the duality gap, while the second minimally perturbs prior estimates of the unspecified parameters to satisfy strong duality, if it is possible to satisfy the optimality conditions exactly. We apply these two approaches to the general linear optimization problem. We also use them to impute unspecified parameters of the uncertainty set for robust linear optimization problems under interval and cardinality constrained uncertainty. Each inverse optimization model we propose is nonconvex, but we show that a globally optimal solution can be obtained either in closed form or by solving a linear number of linear or convex optimization problems.},
	language = {en},
	number = {2},
	urldate = {2023-04-04},
	journal = {European Journal of Operational Research},
	author = {Chan, Timothy C. Y. and Kaw, Neal},
	month = apr,
	year = {2020},
	keywords = {Inverse optimization, Linear programming, Parameter estimation, Robust optimization, invopt, lessrel, read, researchproposal-lessrel},
	pages = {415--427},
}

@misc{bodur_inverse_2021,
	title = {Inverse {Mixed} {Integer} {Optimization}: {Polyhedral} {Insights} and {Trust} {Region} {Methods}},
	shorttitle = {Inverse {Mixed} {Integer} {Optimization}},
	url = {http://arxiv.org/abs/2008.00301},
	doi = {10.48550/arXiv.2008.00301},
	abstract = {Inverse optimization, determining parameters of an optimization problem that render a given solution optimal, has received increasing attention in recent years. While significant inverse optimization literature exists for convex optimization problems, there have been few advances for discrete problems, despite the ubiquity of applications that fundamentally rely on discrete decision-making. In this paper, we present a new set of theoretical insights and algorithms for the general class of inverse mixed integer linear optimization problems. Specifically, a general characterization of optimality conditions is established and leveraged to design new cutting plane solution algorithms. Through an extensive set of computational experiments, we show that our methods provide substantial improvements over existing methods in solving the largest and most difficult instances to date.},
	urldate = {2023-04-04},
	publisher = {arXiv},
	author = {Bodur, Merve and Chan, Timothy C. Y. and Zhu, Ian Yihang},
	month = aug,
	year = {2021},
	note = {arXiv:2008.00301 [math]},
	keywords = {Mathematics - Optimization and Control, invopt, lessrel, read, researchproposal-lessrel},
}

@misc{agrawal_differentiable_2019,
	title = {Differentiable {Convex} {Optimization} {Layers}},
	url = {http://arxiv.org/abs/1910.12430},
	doi = {10.48550/arXiv.1910.12430},
	abstract = {Recent work has shown how to embed differentiable optimization problems (that is, problems whose solutions can be backpropagated through) as layers within deep learning architectures. This method provides a useful inductive bias for certain problems, but existing software for differentiable optimization layers is rigid and difficult to apply to new settings. In this paper, we propose an approach to differentiating through disciplined convex programs, a subclass of convex optimization problems used by domain-specific languages (DSLs) for convex optimization. We introduce disciplined parametrized programming, a subset of disciplined convex programming, and we show that every disciplined parametrized program can be represented as the composition of an affine map from parameters to problem data, a solver, and an affine map from the solver's solution to a solution of the original problem (a new form we refer to as affine-solver-affine form). We then demonstrate how to efficiently differentiate through each of these components, allowing for end-to-end analytical differentiation through the entire convex program. We implement our methodology in version 1.1 of CVXPY, a popular Python-embedded DSL for convex optimization, and additionally implement differentiable layers for disciplined convex programs in PyTorch and TensorFlow 2.0. Our implementation significantly lowers the barrier to using convex optimization problems in differentiable programs. We present applications in linear machine learning models and in stochastic control, and we show that our layer is competitive (in execution time) compared to specialized differentiable solvers from past work.},
	urldate = {2023-04-26},
	publisher = {arXiv},
	author = {Agrawal, Akshay and Amos, Brandon and Barratt, Shane and Boyd, Stephen and Diamond, Steven and Kolter, Zico},
	month = oct,
	year = {2019},
	note = {arXiv:1910.12430 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning, comboptnet, unread},
}

@inproceedings{tan_learning_2020,
	title = {Learning {Linear} {Programs} from {Optimal} {Decisions}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/e44e875c12109e4fa3716c05008048b2-Abstract.html},
	abstract = {We propose a flexible gradient-based framework for learning linear programs from optimal decisions. Linear programs are often specified by hand, using prior knowledge of relevant costs and constraints. In some applications, linear programs must instead be learned from observations of optimal decisions. Learning from optimal decisions is a particularly challenging bilevel problem, and much of the related inverse optimization literature is dedicated to special cases. We tackle the general problem, learning all parameters jointly while allowing flexible parameterizations of costs, constraints, and loss functions. We also address challenges specific to learning linear programs, such as empty feasible regions and non-unique optimal decisions. Experiments show that our method successfully learns synthetic linear programs and minimum-cost multi-commodity flow instances for which previous methods are not directly applicable. We also provide a fast batch-mode PyTorch implementation of the homogeneous interior point algorithm, which supports gradients by implicit differentiation or backpropagation.},
	urldate = {2023-04-12},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Tan, Yingcong and Terekhov, Daria and Delong, Andrew},
	year = {2020},
	keywords = {promising, unread},
	pages = {19738--19749},
}

@misc{teso_machine_2022,
	title = {Machine {Learning} for {Combinatorial} {Optimisation} of {Partially}-{Specified} {Problems}: {Regret} {Minimisation} as a {Unifying} {Lens}},
	shorttitle = {Machine {Learning} for {Combinatorial} {Optimisation} of {Partially}-{Specified} {Problems}},
	url = {http://arxiv.org/abs/2205.10157},
	doi = {10.48550/arXiv.2205.10157},
	abstract = {It is increasingly common to solve combinatorial optimisation problems that are partially-specified. We survey the case where the objective function or the relations between variables are not known or are only partially specified. The challenge is to learn them from available data, while taking into account a set of hard constraints that a solution must satisfy, and that solving the optimisation problem (esp. during learning) is computationally very demanding. This paper overviews four seemingly unrelated approaches, that can each be viewed as learning the objective function of a hard combinatorial optimisation problem: 1) surrogate-based optimisation, 2) empirical model learning, 3) decision-focused learning (`predict + optimise'), and 4) structured-output prediction. We formalise each learning paradigm, at first in the ways commonly found in the literature, and then bring the formalisations together in a compatible way using regret. We discuss the differences and interactions between these frameworks, highlight the opportunities for cross-fertilization and survey open directions.},
	urldate = {2023-04-06},
	publisher = {arXiv},
	author = {Teso, Stefano and Bliek, Laurens and Borghesi, Andrea and Lombardi, Michele and Yorke-Smith, Neil and Guns, Tias and Passerini, Andrea},
	month = may,
	year = {2022},
	note = {arXiv:2205.10157 [cs]},
	keywords = {Computer Science - Machine Learning, promising, unread},
}

@misc{dalle_learning_2022,
	title = {Learning with {Combinatorial} {Optimization} {Layers}: a {Probabilistic} {Approach}},
	shorttitle = {Learning with {Combinatorial} {Optimization} {Layers}},
	url = {http://arxiv.org/abs/2207.13513},
	doi = {10.48550/arXiv.2207.13513},
	abstract = {Combinatorial optimization (CO) layers in machine learning (ML) pipelines are a powerful tool to tackle data-driven decision tasks, but they come with two main challenges. First, the solution of a CO problem often behaves as a piecewise constant function of its objective parameters. Given that ML pipelines are typically trained using stochastic gradient descent, the absence of slope information is very detrimental. Second, standard ML losses do not work well in combinatorial settings. A growing body of research addresses these challenges through diverse methods. Unfortunately, the lack of well-maintained implementations slows down the adoption of CO layers. In this paper, building upon previous works, we introduce a probabilistic perspective on CO layers, which lends itself naturally to approximate differentiation and the construction of structured losses. We recover many approaches from the literature as special cases, and we also derive new ones. Based on this unifying perspective, we present InferOpt.jl, an open-source Julia package that 1) allows turning any CO oracle with a linear objective into a differentiable layer, and 2) defines adequate losses to train pipelines containing such layers. Our library works with arbitrary optimization algorithms, and it is fully compatible with Julia's ML ecosystem. We demonstrate its abilities using a pathfinding problem on video game maps as guiding example, as well as three other applications from operations research.},
	urldate = {2023-04-06},
	publisher = {arXiv},
	author = {Dalle, Guillaume and Baty, Léo and Bouvier, Louis and Parmentier, Axel},
	month = dec,
	year = {2022},
	note = {arXiv:2207.13513 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning, promising, unread},
}

@misc{nandwani_solver-free_2023,
	title = {A {Solver}-{Free} {Framework} for {Scalable} {Learning} in {Neural} {ILP} {Architectures}},
	url = {http://arxiv.org/abs/2210.09082},
	doi = {10.48550/arXiv.2210.09082},
	abstract = {There is a recent focus on designing architectures that have an Integer Linear Programming (ILP) layer within a neural model (referred to as Neural ILP in this paper). Neural ILP architectures are suitable for pure reasoning tasks that require data-driven constraint learning or for tasks requiring both perception (neural) and reasoning (ILP). A recent SOTA approach for end-to-end training of Neural ILP explicitly defines gradients through the ILP black box (Paulus et al. 2021) - this trains extremely slowly, owing to a call to the underlying ILP solver for every training data point in a minibatch. In response, we present an alternative training strategy that is solver-free, i.e., does not call the ILP solver at all at training time. Neural ILP has a set of trainable hyperplanes (for cost and constraints in ILP), together representing a polyhedron. Our key idea is that the training loss should impose that the final polyhedron separates the positives (all constraints satisfied) from the negatives (at least one violated constraint or a suboptimal cost value), via a soft-margin formulation. While positive example(s) are provided as part of the training data, we devise novel techniques for generating negative samples. Our solution is flexible enough to handle equality as well as inequality constraints. Experiments on several problems, both perceptual as well as symbolic, which require learning the constraints of an ILP, show that our approach has superior performance and scales much better compared to purely neural baselines and other state-of-the-art models that require solver-based training. In particular, we are able to obtain excellent performance in 9 x 9 symbolic and visual sudoku, to which the other Neural ILP solver is not able to scale.},
	urldate = {2023-04-06},
	publisher = {arXiv},
	author = {Nandwani, Yatin and Ranjan, Rishabh and Mausam and Singla, Parag},
	month = jan,
	year = {2023},
	note = {arXiv:2210.09082 [cs]},
	keywords = {Computer Science - Machine Learning, promising, unread},
}

@inproceedings{pogancic_differentiation_2020,
	title = {Differentiation of {Blackbox} {Combinatorial} {Solvers}},
	url = {https://openreview.net/forum?id=BkevoJSYPB},
	abstract = {Achieving fusion of deep learning with combinatorial algorithms promises transformative changes to artificial intelligence. One possible approach is to introduce combinatorial building blocks into neural networks. Such end-to-end architectures have the potential to tackle combinatorial problems on raw input data such as ensuring global consistency in multi-object tracking or route planning on maps in robotics. In this work, we present a method that implements an efficient backward pass through blackbox implementations of combinatorial solvers with linear objective functions. We provide both theoretical and experimental backing. In particular, we incorporate the Gurobi MIP solver, Blossom V algorithm, and Dijkstra's algorithm into architectures that extract suitable features from raw inputs for the traveling salesman problem, the min-cost perfect matching problem and the shortest path problem.},
	language = {en},
	urldate = {2023-04-11},
	author = {Pogančić, Marin Vlastelica and Paulus, Anselm and Musil, Vit and Martius, Georg and Rolinek, Michal},
	month = mar,
	year = {2020},
	keywords = {read, researchproposal},
}

@misc{mulamba_discrete_2020,
	title = {Discrete solution pools and noise-contrastive estimation for predict-and-optimize},
	abstract = {Numerous real-life decision-making processes involve solving a combinatorial optimization problem with uncertain input that can be estimated from historic data. There is a growing interest in decision-focused learning methods, where the loss function used for learning to predict the uncertain input uses the outcome of solving the combinatorial problem over a set of predictions. Different surrogate loss functions have been identified, often using a continuous approximation of the combinatorial problem. However, a key bottleneck is that to compute the loss, one has to solve the combinatorial optimisation problem for each training instance in each epoch, which is computationally expensive even in the case of continuous approximations. We propose a different solver-agnostic method for decision-focused learning, namely by considering a pool of feasible solutions as a discrete approximation of the full combinatorial problem. Solving is now trivial through a single pass over the solution pool. We design several variants of a noise-contrastive loss over the solution pool, which we substantiate theoretically and empirically. Furthermore, we show that by dynamically re-solving only a fraction of the training instances each epoch, our method performs on par with the state of the art, whilst drastically reducing the time spent solving, hence increasing the feasibility of predict-and-optimize for larger problems.},
	author = {Mulamba, Maxime and Mandi, Jayanta and Diligenti, Michelangelo and Lombardi, Michele and Bucarey, Víctor and Guns, Tias},
	month = nov,
	year = {2020},
	keywords = {read, researchproposal},
}

@misc{elmachtoub_decision_2020,
	title = {Decision {Trees} for {Decision}-{Making} under the {Predict}-then-{Optimize} {Framework}},
	url = {http://arxiv.org/abs/2003.00360},
	doi = {10.48550/arXiv.2003.00360},
	abstract = {We consider the use of decision trees for decision-making problems under the predict-then-optimize framework. That is, we would like to first use a decision tree to predict unknown input parameters of an optimization problem, and then make decisions by solving the optimization problem using the predicted parameters. A natural loss function in this framework is to measure the suboptimality of the decisions induced by the predicted input parameters, as opposed to measuring loss using input parameter prediction error. This natural loss function is known in the literature as the Smart Predict-then-Optimize (SPO) loss, and we propose a tractable methodology called SPO Trees (SPOTs) for training decision trees under this loss. SPOTs benefit from the interpretability of decision trees, providing an interpretable segmentation of contextual features into groups with distinct optimal solutions to the optimization problem of interest. We conduct several numerical experiments on synthetic and real data including the prediction of travel times for shortest path problems and predicting click probabilities for news article recommendation. We demonstrate on these datasets that SPOTs simultaneously provide higher quality decisions and significantly lower model complexity than other machine learning approaches (e.g., CART) trained to minimize prediction error.},
	urldate = {2023-04-11},
	publisher = {arXiv},
	author = {Elmachtoub, Adam N. and Liang, Jason Cheuk Nam and McNellis, Ryan},
	month = jun,
	year = {2020},
	note = {arXiv:2003.00360 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning, decisiontrees, unread},
}

@inproceedings{mandi_interior_2020,
	title = {Interior {Point} {Solving} for {LP}-based prediction+optimisation},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/51311013e51adebc3c34d2cc591fefee-Abstract.html},
	abstract = {Solving optimization problem is the key to decision making in many real-life analytics applications. However, the coefficients of the optimization problems are often uncertain and dependent on external factors, such as future demand or energy- or stock prices. Machine learning (ML) models, especially neural networks, are increasingly being used to estimate these coefficients in a data-driven way. Hence, end-to-end predict-and-optimize approaches, which consider how effective the predicted values are to solve the optimization problem, have received increasing attention. In case of integer linear programming problems, a popular approach to overcome their non-differentiabilty is to add a quadratic penalty term to the continuous relaxation, such that results from differentiating over quadratic programs can be used. Instead we investigate the use of the more principled logarithmic barrier term, as widely used in interior point solvers for linear programming. Instead of differentiating the KKT conditions, we consider the homogeneous self-dual formulation of the LP and we show the relation between the interior point step direction and corresponding gradients needed for learning. Finally, our empirical experiments demonstrate our approach performs as good as if not better than the state-of-the-art QPTL (Quadratic Programming task loss) formulation of Wilder et al. and SPO approach of Elmachtoub and Grigas.},
	urldate = {2023-04-04},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Mandi, Jayanta and Guns, Tias},
	year = {2020},
	keywords = {read, researchproposal},
	pages = {7272--7282},
}

@misc{vlastelica_differentiation_2020,
	title = {Differentiation of {Blackbox} {Combinatorial} {Solvers}},
	url = {http://arxiv.org/abs/1912.02175},
	doi = {10.48550/arXiv.1912.02175},
	abstract = {Achieving fusion of deep learning with combinatorial algorithms promises transformative changes to artificial intelligence. One possible approach is to introduce combinatorial building blocks into neural networks. Such end-to-end architectures have the potential to tackle combinatorial problems on raw input data such as ensuring global consistency in multi-object tracking or route planning on maps in robotics. In this work, we present a method that implements an efficient backward pass through blackbox implementations of combinatorial solvers with linear objective functions. We provide both theoretical and experimental backing. In particular, we incorporate the Gurobi MIP solver, Blossom V algorithm, and Dijkstra's algorithm into architectures that extract suitable features from raw inputs for the traveling salesman problem, the min-cost perfect matching problem and the shortest path problem. The code is available at https://github.com/martius-lab/blackbox-backprop.},
	urldate = {2023-04-04},
	publisher = {arXiv},
	author = {Vlastelica, Marin and Paulus, Anselm and Musil, Vít and Martius, Georg and Rolínek, Michal},
	month = feb,
	year = {2020},
	note = {arXiv:1912.02175 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, potential, researchproposal, unread},
}

@misc{mandi_smart_2019,
	title = {Smart {Predict}-and-{Optimize} for {Hard} {Combinatorial} {Optimization} {Problems}},
	url = {http://arxiv.org/abs/1911.10092},
	doi = {10.48550/arXiv.1911.10092},
	abstract = {Combinatorial optimization assumes that all parameters of the optimization problem, e.g. the weights in the objective function is fixed. Often, these weights are mere estimates and increasingly machine learning techniques are used to for their estimation. Recently, Smart Predict and Optimize (SPO) has been proposed for problems with a linear objective function over the predictions, more specifically linear programming problems. It takes the regret of the predictions on the linear problem into account, by repeatedly solving it during learning. We investigate the use of SPO to solve more realistic discrete optimization problems. The main challenge is the repeated solving of the optimization problem. To this end, we investigate ways to relax the problem as well as warmstarting the learning and the solving. Our results show that even for discrete problems it often suffices to train by solving the relaxation in the SPO loss. Furthermore, this approach outperforms, for most instances, the state-of-the-art approach of Wilder, Dilkina, and Tambe. We experiment with weighted knapsack problems as well as complex scheduling problems and show for the first time that a predict-and-optimize approach can successfully be used on large-scale combinatorial optimization problems.},
	urldate = {2023-04-04},
	publisher = {arXiv},
	author = {Mandi, Jaynta and Demirović, Emir and Stuckey, Peter J. and Guns, Tias},
	month = nov,
	year = {2019},
	note = {arXiv:1911.10092 [cs, math]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Mathematics - Optimization and Control, researchproposal, unread},
}

@misc{donti_task-based_2019,
	title = {Task-based {End}-to-end {Model} {Learning} in {Stochastic} {Optimization}},
	url = {http://arxiv.org/abs/1703.04529},
	doi = {10.48550/arXiv.1703.04529},
	abstract = {With the increasing popularity of machine learning techniques, it has become common to see prediction algorithms operating within some larger process. However, the criteria by which we train these algorithms often differ from the ultimate criteria on which we evaluate them. This paper proposes an end-to-end approach for learning probabilistic machine learning models in a manner that directly captures the ultimate task-based objective for which they will be used, within the context of stochastic programming. We present three experimental evaluations of the proposed approach: a classical inventory stock problem, a real-world electrical grid scheduling task, and a real-world energy storage arbitrage task. We show that the proposed approach can outperform both traditional modeling and purely black-box policy optimization approaches in these applications.},
	urldate = {2023-04-04},
	publisher = {arXiv},
	author = {Donti, Priya L. and Amos, Brandon and Kolter, J. Zico},
	month = apr,
	year = {2019},
	note = {arXiv:1703.04529 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, researchproposal, unread},
}

@article{ferber_mipaal_2020,
	title = {{MIPaaL}: {Mixed} {Integer} {Program} as a {Layer}},
	volume = {34},
	copyright = {Copyright (c) 2020 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	shorttitle = {{MIPaaL}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/5509},
	doi = {10.1609/aaai.v34i02.5509},
	abstract = {Machine learning components commonly appear in larger decision-making pipelines; however, the model training process typically focuses only on a loss that measures average accuracy between predicted values and ground truth values. Decision-focused learning explicitly integrates the downstream decision problem when training the predictive model, in order to optimize the quality of decisions induced by the predictions. It has been successfully applied to several limited combinatorial problem classes, such as those that can be expressed as linear programs (LP), and submodular optimization. However, these previous applications have uniformly focused on problems with simple constraints. Here, we enable decision-focused learning for the broad class of problems that can be encoded as a mixed integer linear program (MIP), hence supporting arbitrary linear constraints over discrete and continuous variables. We show how to differentiate through a MIP by employing a cutting planes solution approach, an algorithm that iteratively tightens the continuous relaxation by adding constraints removing fractional solutions. We evaluate our new end-to-end approach on several real world domains and show that it outperforms the standard two phase approaches that treat prediction and optimization separately, as well as a baseline approach of simply applying decision-focused learning to the LP relaxation of the MIP. Lastly, we demonstrate generalization performance in several transfer learning tasks.},
	language = {en},
	number = {02},
	urldate = {2023-04-04},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Ferber, Aaron and Wilder, Bryan and Dilkina, Bistra and Tambe, Milind},
	month = apr,
	year = {2020},
	note = {Number: 02},
	keywords = {read, researchproposal},
	pages = {1504--1511},
}

@misc{paulus_comboptnet_2022,
	title = {{CombOptNet}: {Fit} the {Right} {NP}-{Hard} {Problem} by {Learning} {Integer} {Programming} {Constraints}},
	shorttitle = {{CombOptNet}},
	url = {http://arxiv.org/abs/2105.02343},
	doi = {10.48550/arXiv.2105.02343},
	abstract = {Bridging logical and algorithmic reasoning with modern machine learning techniques is a fundamental challenge with potentially transformative impact. On the algorithmic side, many NP-hard problems can be expressed as integer programs, in which the constraints play the role of their "combinatorial specification." In this work, we aim to integrate integer programming solvers into neural network architectures as layers capable of learning both the cost terms and the constraints. The resulting end-to-end trainable architectures jointly extract features from raw data and solve a suitable (learned) combinatorial problem with state-of-the-art integer programming solvers. We demonstrate the potential of such layers with an extensive performance analysis on synthetic data and with a demonstration on a competitive computer vision keypoint matching benchmark.},
	urldate = {2023-04-04},
	publisher = {arXiv},
	author = {Paulus, Anselm and Rolínek, Michal and Musil, Vít and Amos, Brandon and Martius, Georg},
	month = apr,
	year = {2022},
	note = {arXiv:2105.02343 [cs]},
	keywords = {Computer Science - Machine Learning, constraints, read, researchproposal},
}

@misc{yan_surrogate_2021,
	title = {A {Surrogate} {Objective} {Framework} for {Prediction}+{Optimization} with {Soft} {Constraints}},
	url = {http://arxiv.org/abs/2111.11358},
	doi = {10.48550/arXiv.2111.11358},
	abstract = {Prediction+optimization is a common real-world paradigm where we have to predict problem parameters before solving the optimization problem. However, the criteria by which the prediction model is trained are often inconsistent with the goal of the downstream optimization problem. Recently, decision-focused prediction approaches, such as SPO+ and direct optimization, have been proposed to fill this gap. However, they cannot directly handle the soft constraints with the \$max\$ operator required in many real-world objectives. This paper proposes a novel analytically differentiable surrogate objective framework for real-world linear and semi-definite negative quadratic programming problems with soft linear and non-negative hard constraints. This framework gives the theoretical bounds on constraints' multipliers, and derives the closed-form solution with respect to predictive parameters and thus gradients for any variable in the problem. We evaluate our method in three applications extended with soft constraints: synthetic linear programming, portfolio optimization, and resource provisioning, demonstrating that our method outperforms traditional two-staged methods and other decision-focused approaches.},
	urldate = {2023-04-06},
	publisher = {arXiv},
	author = {Yan, Kai and Yan, Jie and Luo, Chuan and Chen, Liting and Lin, Qingwei and Zhang, Dongmei},
	month = nov,
	year = {2021},
	note = {arXiv:2111.11358 [cs, math]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Mathematics - Optimization and Control, promising, unread},
}

@misc{kotary_folded_2023,
	title = {Folded {Optimization} for {End}-to-{End} {Model}-{Based} {Learning}},
	url = {http://arxiv.org/abs/2301.12047},
	doi = {10.48550/arXiv.2301.12047},
	abstract = {The integration of constrained optimization models as components in deep networks has led to promising advances in both these domains. A primary challenge in this setting is backpropagation through the optimization mapping, which typically lacks a closed form. A common approach is unrolling, which relies on automatic differentiation through the operations of an iterative solver. While flexible and general, unrolling can encounter accuracy and efficiency issues in practice. These issues can be avoided by differentiating the optimization mapping analytically, but current frameworks impose rigid requirements on the optimization problem's form. This paper provides theoretical insights into the backpropagation of unrolled optimizers, which lead to a system for generating equivalent but efficiently solvable analytical models. Additionally, it proposes a unifying view of unrolling and analytical differentiation through constrained optimization mappings. Experiments over various structured prediction and decision-focused learning tasks illustrate the potential of the approach both computationally and in terms of enhanced expressiveness.},
	urldate = {2023-04-06},
	publisher = {arXiv},
	author = {Kotary, James and Dinh, My H. and Fioretto, Ferdinando},
	month = jan,
	year = {2023},
	note = {arXiv:2301.12047 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, promising, unread},
}

@misc{institute_for_pure__applied_mathematics_ipam_tias_2023,
	title = {Tias {Guns} - {Prediction} + {Optimisation}: without and with decision-focused learning - {IPAM} at {UCLA}},
	shorttitle = {Tias {Guns} - {Prediction} + {Optimisation}},
	url = {https://www.youtube.com/watch?v=Mauqt0Y1J2I},
	abstract = {Recorded 02 March 2023. Tias Guns of KU Leuven presents "Prediction + Optimisation: without and with decision-focused learning" at IPAM's Artificial Intelligence and Discrete Optimization Workshop.
Abstract: Industry and society are increasingly automating processes, which requires solving combinatorial optimisation problems. To find not just optimal solutions, but also 'desirable' solutions for the end user, it is increasingly important to offer AI tools that automatically learn from the user and the environment and that support the constraint modelling in interpretable ways.
In this talk I will provide an overview of three different ways in which AI can augment the modeling part of combinatorial optimisation. This includes learning from the user (preference learning in VRP), learning from the environment (end-to-end decision focussed learning) and explanation generation, that sit at the intersection of learning and reasoning. As part of this work, we are building a modern constraint programming language called CPMpy(http://cpmpy.readthedocs.io) that eases integration of multiple constraint solving paradigms with machine learning and other scientific python libraries. I will shortly highlight its possibilities beyond the above cases, as well as our larger vision of conversational human-aware technology for optimisation.
Learn more online at: http://www.ipam.ucla.edu/programs/wor...},
	urldate = {2023-04-03},
	author = {{Institute for Pure \& Applied Mathematics (IPAM)}},
	month = mar,
	year = {2023},
	keywords = {tutorial, unread, video},
}

@inproceedings{shah_decision-focused_2022,
	title = {Decision-{Focused} {Learning} without {Decision}-{Making}: {Learning} {Locally} {Optimized} {Decision} {Losses}},
	shorttitle = {Decision-{Focused} {Learning} without {Decision}-{Making}},
	url = {https://openreview.net/forum?id=eN2lQxjWL05},
	abstract = {Decision-Focused Learning (DFL) is a paradigm for tailoring a predictive model to a downstream optimization task that uses its predictions in order to perform better {\textbackslash}textit\{on that specific task\}. The main technical challenge associated with DFL is that it requires being able to differentiate through the optimization problem, which is difficult due to discontinuous solutions and other challenges. Past work has largely gotten around this this issue by {\textbackslash}textit\{handcrafting\} task-specific surrogates to the original optimization problem that provide informative gradients when differentiated through. However, the need to handcraft surrogates for each new task limits the usability of DFL. In addition, there are often no guarantees about the convexity of the resulting surrogates and, as a result, training a predictive model using them can lead to inferior local optima. In this paper, we do away with surrogates altogether and instead {\textbackslash}textit\{learn\} loss functions that capture task-specific information. To the best of our knowledge, ours is the first approach that entirely replaces the optimization component of decision-focused learning with a loss that is automatically learned. Our approach (a) only requires access to a black-box oracle that can solve the optimization problem and is thus {\textbackslash}textit\{generalizable\}, and (b) can be {\textbackslash}textit\{convex by construction\} and so can be easily optimized over. We evaluate our approach on three resource allocation problems from the literature and find that our approach outperforms learning without taking into account task-structure in all three domains, and even hand-crafted surrogates from the literature.},
	language = {en},
	urldate = {2023-04-03},
	author = {Shah, Sanket and Wang, Kai and Wilder, Bryan and Perrault, Andrew and Tambe, Milind},
	month = oct,
	year = {2022},
	keywords = {unread},
}

@article{shah_decision-focused_nodate,
	title = {Decision-{Focused} {Learning} without {Differentiable} {Optimization}: {Learning} {Locally} {Optimized} {Decision} {Losses}},
	abstract = {Decision-Focused Learning (DFL) is a paradigm for tailoring a predictive model to a downstream optimization task that uses its predictions in order to perform better on that speciﬁc task. The main technical challenge associated with DFL is that it requires being able to differentiate through the optimization problem, which is difﬁcult due to discontinuous solutions and other challenges. Past work has largely gotten around this this issue by handcrafting task-speciﬁc surrogates to the original optimization problem that provide informative gradients when differentiated through. However, the need to handcraft surrogates for each new task limits the usability of DFL. In addition, there are often no guarantees about the convexity of the resulting surrogates and, as a result, training a predictive model using them can lead to inferior local optima. In this paper, we do away with surrogates altogether and instead learn loss functions that capture task-speciﬁc information. To the best of our knowledge, ours is the ﬁrst approach that entirely replaces the optimization component of decision-focused learning with a loss that is automatically learned. Our approach (a) only requires access to a black-box oracle that can solve the optimization problem and is thus generalizable, and (b) can be convex by construction and so can be easily optimized over. We evaluate our approach on three resource allocation problems from the literature and ﬁnd that our approach outperforms learning without taking into account task-structure in all three domains, and even hand-crafted surrogates from the literature.},
	language = {en},
	author = {Shah, Sanket and Wang, Kai and Wilder, Bryan and Perrault, Andrew and Tambe, Milind},
}

@inproceedings{mayte_case_2022,
	title = {Case {Study}: {Applying} {Decision} {Focused} {Learning} in the {Real} {World}},
	shorttitle = {Case {Study}},
	author = {Mayte, Aditya and Taneja, Aparna and Wang, Kai and Tambe, Milind Shashikant and Verma, Shresth},
	year = {2022},
	keywords = {unread},
}

@inproceedings{donti_task-based_2017,
	title = {Task-based {End}-to-end {Model} {Learning} in {Stochastic} {Optimization}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper/2017/hash/3fc2c60b5782f641f76bcefc39fb2392-Abstract.html},
	abstract = {With the increasing popularity of machine learning techniques, it has become common to see prediction algorithms operating within some larger process. However, the criteria by which we train these algorithms often differ from the ultimate criteria on which we evaluate them. This paper proposes an end-to-end approach for learning probabilistic machine learning models in a manner that directly captures the ultimate task-based objective for which they will be used, within the context of stochastic programming. We present three experimental evaluations of the proposed approach: a classical inventory stock problem, a real-world electrical grid scheduling task, and a real-world energy storage arbitrage task. We show that the proposed approach can outperform both traditional modeling and purely black-box policy optimization approaches in these applications.},
	urldate = {2023-04-03},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Donti, Priya and Amos, Brandon and Kolter, J. Zico},
	year = {2017},
	keywords = {unread},
}

@misc{balghiti_generalization_2022,
	title = {Generalization {Bounds} in the {Predict}-then-{Optimize} {Framework}},
	url = {http://arxiv.org/abs/1905.11488},
	doi = {10.48550/arXiv.1905.11488},
	abstract = {The predict-then-optimize framework is fundamental in many practical settings: predict the unknown parameters of an optimization problem, and then solve the problem using the predicted values of the parameters. A natural loss function in this environment is to consider the cost of the decisions induced by the predicted parameters, in contrast to the prediction error of the parameters. This loss function was recently introduced in Elmachtoub and Grigas (2022) and referred to as the Smart Predict-then-Optimize (SPO) loss. In this work, we seek to provide bounds on how well the performance of a prediction model fit on training data generalizes out-of-sample, in the context of the SPO loss. Since the SPO loss is non-convex and non-Lipschitz, standard results for deriving generalization bounds do not apply. We first derive bounds based on the Natarajan dimension that, in the case of a polyhedral feasible region, scale at most logarithmically in the number of extreme points, but, in the case of a general convex feasible region, have linear dependence on the decision dimension. By exploiting the structure of the SPO loss function and a key property of the feasible region, which we denote as the strength property, we can dramatically improve the dependence on the decision and feature dimensions. Our approach and analysis rely on placing a margin around problematic predictions that do not yield unique optimal solutions, and then providing generalization bounds in the context of a modified margin SPO loss function that is Lipschitz continuous. Finally, we characterize the strength property and show that the modified SPO loss can be computed efficiently for both strongly convex bodies and polytopes with an explicit extreme point representation.},
	urldate = {2023-04-03},
	publisher = {arXiv},
	author = {Balghiti, Othman El and Elmachtoub, Adam N. and Grigas, Paul and Tewari, Ambuj},
	month = aug,
	year = {2022},
	note = {Number: arXiv:1905.11488
arXiv:1905.11488 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, unread},
}

@misc{grigas_integrated_2022,
	title = {Integrated {Conditional} {Estimation}-{Optimization}},
	url = {http://arxiv.org/abs/2110.12351},
	doi = {10.48550/arXiv.2110.12351},
	abstract = {Many real-world optimization problems involve uncertain parameters with probability distributions that can be estimated using contextual feature information. In contrast to the standard approach of first estimating the distribution of uncertain parameters and then optimizing the objective based on the estimation, we propose an {\textbackslash}textit\{integrated conditional estimation-optimization\} (ICEO) framework that estimates the underlying conditional distribution of the random parameter while considering the structure of the optimization problem. We directly model the relationship between the conditional distribution of the random parameter and the contextual features, and then estimate the probabilistic model with an objective that aligns with the downstream optimization problem. We show that our ICEO approach is asymptotically consistent under moderate regularity conditions and further provide finite performance guarantees in the form of generalization bounds. Computationally, performing estimation with the ICEO approach is a non-convex and often non-differentiable optimization problem. We propose a general methodology for approximating the potentially non-differentiable mapping from estimated conditional distribution to optimal decision by a differentiable function, which greatly improves the performance of gradient-based algorithms applied to the non-convex problem. We also provide a polynomial optimization solution approach in the semi-algebraic case. Numerical experiments are also conducted to show the empirical success of our approach in different situations including with limited data samples and model mismatches.},
	urldate = {2023-04-03},
	publisher = {arXiv},
	author = {Grigas, Paul and Qi, Meng and Zuo-Jun and Shen},
	month = jun,
	year = {2022},
	note = {Number: arXiv:2110.12351
arXiv:2110.12351 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, unread},
}

@misc{laage_periodic_2022-1,
	title = {Periodic {Freight} {Demand} {Estimation} for {Large}-scale {Tactical} {Planning}},
	url = {http://arxiv.org/abs/2105.09136},
	doi = {10.48550/arXiv.2105.09136},
	abstract = {Freight carriers rely on tactical planning to design their service network to satisfy demand in a cost-effective way. For computational tractability, deterministic and cyclic Service Network Design (SND) formulations are used to solve large-scale problems. A central input is the periodic demand, that is, the demand expected to repeat in every period in the planning horizon. In practice, demand is predicted by a time series forecasting model and the periodic demand is the average of those forecasts. This is, however, only one of many possible mappings. The problem consisting in selecting this mapping has hitherto been overlooked in the literature. We propose to use the structure of the downstream decision-making problem to select a good mapping. For this purpose, we introduce a multilevel mathematical programming formulation that explicitly links the time series forecasts to the SND problem of interest. The solution is a periodic demand estimate that minimizes costs over the tactical planning horizon. We report results in an extensive empirical study of a large-scale application from the Canadian National Railway Company. They clearly show the importance of the periodic demand estimation problem. Indeed, the planning costs exhibit an important variation over different periodic demand estimates and using an estimate different from the mean forecast can lead to substantial cost reductions. Moreover, the costs associated with the periodic demand estimates based on forecasts were comparable to, or even better than those obtained using the mean of actual demand.},
	urldate = {2023-04-03},
	publisher = {arXiv},
	author = {Laage, Greta and Frejinger, Emma and Savard, Gilles},
	month = jan,
	year = {2022},
	note = {Number: arXiv:2105.09136
arXiv:2105.09136 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, prethesis},
}

@misc{kotary_end--end_2021,
	title = {End-to-{End} {Constrained} {Optimization} {Learning}: {A} {Survey}},
	shorttitle = {End-to-{End} {Constrained} {Optimization} {Learning}},
	url = {http://arxiv.org/abs/2103.16378},
	doi = {10.48550/arXiv.2103.16378},
	abstract = {This paper surveys the recent attempts at leveraging machine learning to solve constrained optimization problems. It focuses on surveying the work on integrating combinatorial solvers and optimization methods with machine learning architectures. These approaches hold the promise to develop new hybrid machine learning and optimization methods to predict fast, approximate, solutions to combinatorial problems and to enable structural logical inference. This paper presents a conceptual review of the recent advancements in this emerging area.},
	urldate = {2023-04-03},
	publisher = {arXiv},
	author = {Kotary, James and Fioretto, Ferdinando and Van Hentenryck, Pascal and Wilder, Bryan},
	month = mar,
	year = {2021},
	note = {Number: arXiv:2103.16378
arXiv:2103.16378 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, prethesis},
}

@misc{larsen_fast_2022,
	title = {Fast {Continuous} and {Integer} {L}-shaped {Heuristics} {Through} {Supervised} {Learning}},
	url = {http://arxiv.org/abs/2205.00897},
	doi = {10.48550/arXiv.2205.00897},
	abstract = {We propose a methodology at the nexus of operations research and machine learning (ML) leveraging generic approximators available from ML to accelerate the solution of mixed-integer linear two-stage stochastic programs. We aim at solving problems where the second stage is highly demanding. Our core idea is to gain large reductions in online solution time while incurring small reductions in first-stage solution accuracy by substituting the exact second-stage solutions with fast, yet accurate supervised ML predictions. This upfront investment in ML would be justified when similar problems are solved repeatedly over time, for example, in transport planning related to fleet management, routing and container yard management. Our numerical results focus on the problem class seminally addressed with the integer and continuous L-shaped cuts. Our extensive empirical analysis is grounded in standardized families of problems derived from stochastic server location (SSLP) and stochastic multi knapsack (SMKP) problems available in the literature. The proposed method can solve the hardest instances of SSLP in less than 9\% of the time it takes the state-of-the-art exact method, and in the case of SMKP the same figure is 20\%. Average optimality gaps are in most cases less than 0.1\%.},
	urldate = {2023-04-03},
	publisher = {arXiv},
	author = {Larsen, Eric and Frejinger, Emma and Gendron, Bernard and Lodi, Andrea},
	month = jun,
	year = {2022},
	note = {Number: arXiv:2205.00897
arXiv:2205.00897 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, prethesis},
}

@article{larsen_predicting_2021,
	title = {Predicting {Tactical} {Solutions} to {Operational} {Planning} {Problems} {Under} {Imperfect} {Information}},
	copyright = {Copyright © 2021, INFORMS},
	url = {https://pubsonline.informs.org/doi/abs/10.1287/ijoc.2021.1091},
	doi = {10.1287/ijoc.2021.1091},
	abstract = {This paper offers a methodological contribution at the intersection of machine learning and operations research. Namely, we propose a methodology to quickly predict expected tactical descriptions o...},
	language = {EN},
	urldate = {2023-04-03},
	journal = {INFORMS Journal on Computing},
	author = {Larsen, Eric and Lachapelle, Sébastien and Bengio, Yoshua and Frejinger, Emma and Lacoste-Julien, Simon and Lodi, Andrea},
	month = sep,
	year = {2021},
	note = {Publisher: INFORMS},
	keywords = {prethesis},
}

@misc{noauthor_travel_nodate,
	title = {Travel {Forecasting} {Resource}},
	url = {https://tfresource.org},
	abstract = {Travel forecasting, explained. A collection of best practices and practical know-how for learning about, creating, and using travel forecasting models.},
	language = {en-US},
	urldate = {2022-03-17},
	journal = {Travel Forecasting, Explained},
}

@misc{noauthor_mathematical_nodate,
	title = {Mathematical programming versus constraint programming — {IBM}® {Decision} {Optimization} {CPLEX}® {Modeling} for {Python} ({DOcplex}) {V2}.22 documentation},
	url = {https://ibmdecisionoptimization.github.io/docplex-doc/mp_vs_cp.html},
	urldate = {2022-03-17},
}

@article{hooker_logic_2002,
	title = {Logic, {Optimization}, and {Constraint} {Programming}},
	volume = {14},
	issn = {1091-9856, 1526-5528},
	url = {http://pubsonline.informs.org/doi/abs/10.1287/ijoc.14.4.295.2828},
	doi = {10.1287/ijoc.14.4.295.2828},
	language = {en},
	number = {4},
	urldate = {2022-03-17},
	journal = {INFORMS Journal on Computing},
	author = {Hooker, John N.},
	month = nov,
	year = {2002},
	pages = {295--321},
}

@article{unsal_constraint_2013,
	title = {Constraint programming approach to quay crane scheduling problem},
	volume = {59},
	issn = {13665545},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1366554513001543},
	doi = {10.1016/j.tre.2013.08.006},
	abstract = {This study presents a constraint programming (CP) model for the quay crane scheduling problem (QCSP), which occurs at container terminals, with realistic constraints such as safety margins, travel times and precedence relations. Next, QCSP with time windows and integrated crane assignment and scheduling problem, are discussed. The performance of the CP model is compared with that of algorithms presented in QCSP literature. The results of the computational experiments indicate that the CP model is able to produce good results while reducing the computational time, and is a robust and ﬂexible alternative for different types of crane scheduling problems.},
	language = {en},
	urldate = {2022-03-17},
	journal = {Transportation Research Part E: Logistics and Transportation Review},
	author = {Unsal, Ozgur and Oguz, Ceyda},
	month = nov,
	year = {2013},
	pages = {108--122},
}

@article{gur_surgical_2019,
	title = {Surgical {Operation} {Scheduling} with {Goal} {Programming} and {Constraint} {Programming}: {A} {Case} {Study}},
	volume = {7},
	issn = {2227-7390},
	shorttitle = {Surgical {Operation} {Scheduling} with {Goal} {Programming} and {Constraint} {Programming}},
	url = {https://www.mdpi.com/2227-7390/7/3/251},
	doi = {10.3390/math7030251},
	abstract = {The achievement of health organizations’ goals is critically important for proﬁtability. For this purpose, their resources, materials, and equipment should be efﬁciently used in the services they provide. A hospital has sensitive and expensive equipment, and the use of its equipment and resources needs to be balanced. The utilization of these resources should be considered in its operating rooms, as it shares both expense expenditure and revenue generation. This study’s primary aim is the effective and balanced use of equipment and resources in hospital operating rooms. In this context, datasets from a state hospital were used via the goal programming and constraint programming methods. According to the wishes of hospital managers, three scenarios were separately modeled in both methods. According to the obtained results, schedules were compared and analyzed according to the current situation. The hospital-planning approach was positively affected, and goals such as minimization cost, staff and patient satisfaction, prevention over time, and less use were achieved.},
	language = {en},
	number = {3},
	urldate = {2022-03-17},
	journal = {Mathematics},
	author = {Gür, Şeyda and Eren, Tamer and Alakaş, Hacı},
	month = mar,
	year = {2019},
	pages = {251},
}

@article{weil_constraint_1995,
	title = {Constraint programming for nurse scheduling},
	volume = {14},
	issn = {07395175},
	url = {http://ieeexplore.ieee.org/document/395324/},
	doi = {10.1109/51.395324},
	language = {en},
	number = {4},
	urldate = {2022-03-17},
	journal = {IEEE Engineering in Medicine and Biology Magazine},
	author = {Weil, G. and Heus, K. and Francois, P. and Poujade, M.},
	month = aug,
	year = {1995},
	pages = {417--422},
}

@article{rodriguez_constraint_2007,
	title = {A constraint programming model for real-time train scheduling at junctions},
	volume = {41},
	issn = {01912615},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0191261506000233},
	doi = {10.1016/j.trb.2006.02.006},
	abstract = {In this paper, we present a constraint programming model for the routing and scheduling of trains running through a junction. The model uses input data from relevant time events of train runs calculated by a simulator. The model can be integrated into a decision support system used by operators who make decisions to change train routes or orders to avoid conﬂicts and delays. The model has been applied to a set of problem instances. This set has been deﬁned from a real case study of traﬃc on the Pierreﬁtte-Gonesse node, North of Paris. Preliminary results show that the solution identiﬁed by the model yields a signiﬁcant improvement in performance within an acceptable computation time.},
	language = {en},
	number = {2},
	urldate = {2022-03-17},
	journal = {Transportation Research Part B: Methodological},
	author = {Rodriguez, Joaquín},
	month = feb,
	year = {2007},
	pages = {231--245},
}

@article{refanidis_constraint-based_2010,
	title = {A constraint-based approach to scheduling an individual's activities},
	volume = {1},
	issn = {2157-6904, 2157-6912},
	url = {https://dl.acm.org/doi/10.1145/1869397.1869401},
	doi = {10.1145/1869397.1869401},
	abstract = {The goal of helping to automate the management of an individual's time is ambitious in terms both of knowledge engineering and of the quality of the plans produced by an AI system. Modeling an individual's activities is itself a challenge, due to the variety of activity, constraint, and preference types involved. Activities might be simple or interruptible; they might have fixed or variable durations, constraints over their temporal domains, and binary constraints between them. Activities might require the individual being at specific locations in order, whereas traveling time should be taken into account. Some activities might require exclusivity, whereas others can be overlapped with compatible concurrent activities. Finally, while scheduled activities generate utility for the individual, extra utility might result from the way activities are scheduled in time, individually and in conjunction.
            This article presents a rigorous, expressive model to represent an individual's activities, that is, activities whose scheduling is not contingent on any other person. Joint activities such as meetings are outside our remit; it is expected that these are arranged manually or through negotiation mechanisms and they are considered as fixed busy times in the individual's calendar. The model, formulated as a constraint optimization problem, is general enough to accommodate a variety of situations. We present a scheduler that operates on this rich model, based on the general squeaky wheel optimization framework and enhanced with domain-dependent heuristics and forward checking. Our empirical evaluation demonstrates both the efficiency and the effectiveness of the selected approach. Part of the work described has been implemented in the SelfPlanner system, a Web-based intelligent calendar application that utilizes Google Calendar.},
	language = {en},
	number = {2},
	urldate = {2022-03-17},
	journal = {ACM Transactions on Intelligent Systems and Technology},
	author = {Refanidis, Ioannis and Yorke-Smith, Neil},
	month = nov,
	year = {2010},
	pages = {1--32},
}

@misc{pape_constraint-based_2005,
	title = {Constraint-{Based} {Scheduling} : {A} {Tutorial}},
	shorttitle = {Constraint-{Based} {Scheduling}},
	url = {https://www.semanticscholar.org/paper/Constraint-Based-Scheduling-%3A-A-Tutorial-Pape/366a90f5842091a48d51920808b9c3af5c2a46b7},
	abstract = {Given a set of resources with given capacities, a set of activities with given processing times and resource requirements, and a set of temporal constraints between activities, a “pure” scheduling problem consists of deciding when to execute each activity, so that both temporal constraints and resource constraints are satisfied. Most scheduling problems can easily be represented as instances of the constraint satisfaction problem (Kumar, 1992): given a set of variables, a set of possible values (domain) for each variable, and a set of constraints between the variables, assign a value to each variable, so that all the constraints are satisfied.},
	language = {en},
	urldate = {2022-03-17},
	author = {Pape, C. L.},
	year = {2005},
}

@book{castiglione_activity-based_2015,
	address = {Washington, DC},
	title = {Activity-based travel demand models: a primer},
	isbn = {978-0-309-27399-2},
	shorttitle = {Activity-based travel demand models},
	language = {en},
	publisher = {Transportation Research Board},
	author = {Castiglione, Joe},
	year = {2015},
}

@article{noauthor_notitle_nodate,
}
